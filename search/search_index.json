{
    "docs": [
        {
            "location": "/", 
            "text": "WekaDeeplearning4J: Deep Learning using Weka\n\n\nWekaDeeplearning4J is a deep learning package for the \nWeka\n workbench. It is developed to incorporate the modern techniques of deep learning into Weka. The backend is provided by the \nDeeplearning4J\n Java library. \n\n\nThe source code for this package is available on \nGitHub\n. The java-doc can be found \nhere\n.\n\n\n\n\nThis documentation is still work in progress.\n\n\nFunctionality\n\n\nAll functionality of this package is accessible via the Weka GUI, the commandline and programmatically in Java.\n\n\nThe following Neural Network Layers are available to build sophisticated architectures:\n\n\n\n\nConvolutionLayer\n: applying convolution, useful for images and text embeddings\n\n\nDenseLayer\n: all units are connected to all units of its parent layer\n\n\nSubsamplingLayer\n: subsample from groups of units of the parent layer by different strategies (average, maximum, etc.)\n\n\nBatchNormalization\n: applies the common batch normalization strategy on the activations of the parent layer\n\n\nOutputLayer\n: generates \nN\n outputs based on a given activation function", 
            "title": "Home"
        }, 
        {
            "location": "/#wekadeeplearning4j-deep-learning-using-weka", 
            "text": "WekaDeeplearning4J is a deep learning package for the  Weka  workbench. It is developed to incorporate the modern techniques of deep learning into Weka. The backend is provided by the  Deeplearning4J  Java library.   The source code for this package is available on  GitHub . The java-doc can be found  here .   This documentation is still work in progress.", 
            "title": "WekaDeeplearning4J: Deep Learning using Weka"
        }, 
        {
            "location": "/#functionality", 
            "text": "All functionality of this package is accessible via the Weka GUI, the commandline and programmatically in Java.  The following Neural Network Layers are available to build sophisticated architectures:   ConvolutionLayer : applying convolution, useful for images and text embeddings  DenseLayer : all units are connected to all units of its parent layer  SubsamplingLayer : subsample from groups of units of the parent layer by different strategies (average, maximum, etc.)  BatchNormalization : applies the common batch normalization strategy on the activations of the parent layer  OutputLayer : generates  N  outputs based on a given activation function", 
            "title": "Functionality"
        }, 
        {
            "location": "/install/", 
            "text": "Prerequisites\n\n\n\n\nWeka 3.8.1 or above (\nhere\n)\n\n\nWekaDeeplearning4j package 1.2 or above (\nhere\n)\n\n\n\n\nYou need to unzip the Weka zip file to a directory of your choice.\n\n\nCPU\n\n\nFor the CPU package no further requisites are necessary.\n\n\nGPU\n\n\nThe GPU package needs the CUDA 8.0 backend to be installed on your system. Nvidia provides some good installation instructions for all platforms:\n\n\n\n\nLinux\n\n\nMac OS X\n\n\nWindows\n\n\n\n\nInstalling the Weka Package\n\n\nWeka packages can be easily installed either via the user interface as described \nhere\n, or simply via the commandline:\n\n\n$ java -cp \nWEKA-JAR-PATH\n weka.core.WekaPackageManager \\\n       -install-package wekaDeeplearning4j\nBACKEND\n-dev.zip\n\n\n\n\nwhere \nWEKA-JAR-PATH\n must be replaced by the path pointing to the Weka jar file and \nBACKEND\n must be replaced by either \nCPU\n or \nGPU\n, depending on which version you chose.\n\n\nYou can check whether the installation was successful with\n\n\n$ java -cp \nWEKA-JAR-PATH\n weka.core.WekaPackageManager \\\n       -list-packages installed\n\n\n\n\nwhich results in\n\n\nInstalled   Repository  Loaded  Package\n=========   ==========  ======  =======\n1.2.0       -----       Yes     wekaDeeplearning4j\nBACKEND\n-dev: Weka wrappers for Deeplearning4j", 
            "title": "Installation"
        }, 
        {
            "location": "/install/#prerequisites", 
            "text": "Weka 3.8.1 or above ( here )  WekaDeeplearning4j package 1.2 or above ( here )   You need to unzip the Weka zip file to a directory of your choice.", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/install/#cpu", 
            "text": "For the CPU package no further requisites are necessary.", 
            "title": "CPU"
        }, 
        {
            "location": "/install/#gpu", 
            "text": "The GPU package needs the CUDA 8.0 backend to be installed on your system. Nvidia provides some good installation instructions for all platforms:   Linux  Mac OS X  Windows", 
            "title": "GPU"
        }, 
        {
            "location": "/install/#installing-the-weka-package", 
            "text": "Weka packages can be easily installed either via the user interface as described  here , or simply via the commandline:  $ java -cp  WEKA-JAR-PATH  weka.core.WekaPackageManager \\\n       -install-package wekaDeeplearning4j BACKEND -dev.zip  where  WEKA-JAR-PATH  must be replaced by the path pointing to the Weka jar file and  BACKEND  must be replaced by either  CPU  or  GPU , depending on which version you chose.  You can check whether the installation was successful with  $ java -cp  WEKA-JAR-PATH  weka.core.WekaPackageManager \\\n       -list-packages installed  which results in  Installed   Repository  Loaded  Package\n=========   ==========  ======  =======\n1.2.0       -----       Yes     wekaDeeplearning4j BACKEND -dev: Weka wrappers for Deeplearning4j", 
            "title": "Installing the Weka Package"
        }, 
        {
            "location": "/getting-started/", 
            "text": "Usage\n\n\nIf you are new to Weka, you should probably first start reading the \nWeka primer\n as a basic introduction.\n\n\nAs most of Weka, the WekaDeeplearning4j's functionality is accessible in three ways:\n\n\n\n\nVia the commandline interface\n\n\nProgramming with Weka in Java\n\n\nUsing the Weka workbench GUI\n\n\n\n\nAll three ways are explained in the following. The main classifier exposed by this package is named \nDl4jMlpClassifier\n.\nSimple examples are given in the examples section for the \nIris dataset\n and the \nMNIST dataset\n.\n\n\nCommandline Interface\n\n\nA first look for the available commandline options of the \nDl4jMlpClassifier\n is shown with\n\n\n$ java -cp weka.jar weka.Run .Dl4jMlpClassifier -h\n\n\n\n\nBelow the general options, the specific ones are listed:\n\n\nOptions specific to weka.classifiers.functions.Dl4jMlpClassifier:\n\n-S \nnum\n\n    Random number seed.\n    (default 1)\n-logFile \nstring\n\n    The name of the log file to write loss information to (default = no log file).\n-layer \nstring\n\n    The specification of a layer. This option can be used multiple times.\n-numEpochs \nint\n\n    The number of epochs to perform.\n-iterator \nstring\n\n    The dataset iterator to use.\n-config \nstring\n\n    The neural network configuration to use.\n-normalization \nint\n\n    The type of normalization to perform.\n-queueSize \nint\n\n    The queue size for asynchronous data transfer (default: 0, synchronous transfer).\n-output-debug-info\n    If set, classifier is run in debug mode and\n    may output additional info to the console\n-do-not-check-capabilities\n    If set, classifier capabilities are not checked before classifier is built\n    (use with caution).\n-num-decimal-places\n    The number of decimal places for the output of numbers in the model (default 2).\n-batch-size\n    The desired batch size for batch prediction  (default 100).\n\n\n\n\nThe most interesting option may be the \n-layer\n specification. This option can be used multiple times and defines the architecture of the network layer-wise. \n\n\n$ java -cp weka.jar weka.Run \\\n       .Dl4jMlpClassifier \\\n       -layer \nweka.dl4j.layers.DenseLayer \\\n              -activation weka.dl4j.activations.ActivationReLU \\\n              -nOut 10\n \\\n       -layer \nweka.dl4j.layers.OutputLayer \\\n              -activation weka.dl4j.activations.ActivationSoftmax \\\n              -lossFn weka.dl4j.lossfunctions.LossMCXENT\n \n\n\n\n\nThe above setup builds a network with one hidden layer, having 10 output units using the ReLU activation function, followed by an output layer with the softmax activation function, using a multi-class cross-entropy loss function (MCXENT) as optimization objective.\n\n\nAnother important option is the neural network configuration \n-conf\n in which you can setup hyperparameters for the network. Available options can be found in the \nJava documentation\n (the field \ncommandLineParamSynopsis\n indicates the commandline parameter name for each available method).\n\n\nJava\n\n\nThe Java API is a straight forward wrapper for the official DeepLearning4j API. Using the \nDl4jMlPClassifier\n your code should usually start with\n\n\n// Create a new Multi-Layer-Perceptron classifier\nDl4jMlpClassifier clf = new Dl4jMlpClassifier();\n\n\n\n\nThe networks architecture can be set up by creating each layer step by step:\n\n\nDenseLayer denseLayer = new DenseLayer();\ndenseLayer.setNOut(10);\ndenseLayer.setActivationFn(new ActivationReLU());\ndenseLayer.setWeightInit(WeightInit.XAVIER);\n\n// Define the output layer\nOutputLayer outputLayer = new OutputLayer();\noutputLayer.setActivationFn(new ActivationSoftmax());\noutputLayer.setUpdater(Updater.SGD);\noutputLayer.setLearningRate(0.01);\noutputLayer.setBiasLearningRate(0.01);\n\n\n\n\nFurther configuration can be done by setting a \nNeuralNetConfiguration\n\n\nNeuralNetConfiguration nnc = new NeuralNetConfiguration();\nnnc.setOptimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT);\nclf.setNeuralNetConfiguration(nnc);\n\n\n\n\nFinally the layers are set with\n\n\n// Add the layers to the classifier\nclf.setLayers(new Layer[]{denseLayer, outputLayer});\n\n\n\n\nGUI\n\n\nA tutorial on how to use the GUI is coming soon.\n\n\nModel Zoo\n\n\nWekaDeeplearning4j adapts the modelzoo of Deeplearning4j. That means it is possible to load predefined architectures as neural network and train it on a new dataset. Currently implemented architectures are:\n\n\n\n\nAlexNet\n\n\nLeNet\n\n\nSimpleCNN\n\n\nVGG16\n\n\nVGG19\n\n\n\n\nThis set of models will be extended over the time.\n\n\nTo set a predefined model, e.g. LeNet, from the modelzoo, it is necessary to add the \n-zooModel \"weka.dl4j.zoo.LeNet\"\n option via commandline, or call the \nsetZooModel(new LeNet())\n on the \nDl4jMlpClassifier\n.", 
            "title": "Getting Started"
        }, 
        {
            "location": "/getting-started/#usage", 
            "text": "If you are new to Weka, you should probably first start reading the  Weka primer  as a basic introduction.  As most of Weka, the WekaDeeplearning4j's functionality is accessible in three ways:   Via the commandline interface  Programming with Weka in Java  Using the Weka workbench GUI   All three ways are explained in the following. The main classifier exposed by this package is named  Dl4jMlpClassifier .\nSimple examples are given in the examples section for the  Iris dataset  and the  MNIST dataset .", 
            "title": "Usage"
        }, 
        {
            "location": "/getting-started/#commandline-interface", 
            "text": "A first look for the available commandline options of the  Dl4jMlpClassifier  is shown with  $ java -cp weka.jar weka.Run .Dl4jMlpClassifier -h  Below the general options, the specific ones are listed:  Options specific to weka.classifiers.functions.Dl4jMlpClassifier:\n\n-S  num \n    Random number seed.\n    (default 1)\n-logFile  string \n    The name of the log file to write loss information to (default = no log file).\n-layer  string \n    The specification of a layer. This option can be used multiple times.\n-numEpochs  int \n    The number of epochs to perform.\n-iterator  string \n    The dataset iterator to use.\n-config  string \n    The neural network configuration to use.\n-normalization  int \n    The type of normalization to perform.\n-queueSize  int \n    The queue size for asynchronous data transfer (default: 0, synchronous transfer).\n-output-debug-info\n    If set, classifier is run in debug mode and\n    may output additional info to the console\n-do-not-check-capabilities\n    If set, classifier capabilities are not checked before classifier is built\n    (use with caution).\n-num-decimal-places\n    The number of decimal places for the output of numbers in the model (default 2).\n-batch-size\n    The desired batch size for batch prediction  (default 100).  The most interesting option may be the  -layer  specification. This option can be used multiple times and defines the architecture of the network layer-wise.   $ java -cp weka.jar weka.Run \\\n       .Dl4jMlpClassifier \\\n       -layer  weka.dl4j.layers.DenseLayer \\\n              -activation weka.dl4j.activations.ActivationReLU \\\n              -nOut 10  \\\n       -layer  weka.dl4j.layers.OutputLayer \\\n              -activation weka.dl4j.activations.ActivationSoftmax \\\n              -lossFn weka.dl4j.lossfunctions.LossMCXENT    The above setup builds a network with one hidden layer, having 10 output units using the ReLU activation function, followed by an output layer with the softmax activation function, using a multi-class cross-entropy loss function (MCXENT) as optimization objective.  Another important option is the neural network configuration  -conf  in which you can setup hyperparameters for the network. Available options can be found in the  Java documentation  (the field  commandLineParamSynopsis  indicates the commandline parameter name for each available method).", 
            "title": "Commandline Interface"
        }, 
        {
            "location": "/getting-started/#java", 
            "text": "The Java API is a straight forward wrapper for the official DeepLearning4j API. Using the  Dl4jMlPClassifier  your code should usually start with  // Create a new Multi-Layer-Perceptron classifier\nDl4jMlpClassifier clf = new Dl4jMlpClassifier();  The networks architecture can be set up by creating each layer step by step:  DenseLayer denseLayer = new DenseLayer();\ndenseLayer.setNOut(10);\ndenseLayer.setActivationFn(new ActivationReLU());\ndenseLayer.setWeightInit(WeightInit.XAVIER);\n\n// Define the output layer\nOutputLayer outputLayer = new OutputLayer();\noutputLayer.setActivationFn(new ActivationSoftmax());\noutputLayer.setUpdater(Updater.SGD);\noutputLayer.setLearningRate(0.01);\noutputLayer.setBiasLearningRate(0.01);  Further configuration can be done by setting a  NeuralNetConfiguration  NeuralNetConfiguration nnc = new NeuralNetConfiguration();\nnnc.setOptimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT);\nclf.setNeuralNetConfiguration(nnc);  Finally the layers are set with  // Add the layers to the classifier\nclf.setLayers(new Layer[]{denseLayer, outputLayer});", 
            "title": "Java"
        }, 
        {
            "location": "/getting-started/#gui", 
            "text": "A tutorial on how to use the GUI is coming soon.", 
            "title": "GUI"
        }, 
        {
            "location": "/getting-started/#model-zoo", 
            "text": "WekaDeeplearning4j adapts the modelzoo of Deeplearning4j. That means it is possible to load predefined architectures as neural network and train it on a new dataset. Currently implemented architectures are:   AlexNet  LeNet  SimpleCNN  VGG16  VGG19   This set of models will be extended over the time.  To set a predefined model, e.g. LeNet, from the modelzoo, it is necessary to add the  -zooModel \"weka.dl4j.zoo.LeNet\"  option via commandline, or call the  setZooModel(new LeNet())  on the  Dl4jMlpClassifier .", 
            "title": "Model Zoo"
        }, 
        {
            "location": "/examples/classifying-iris/", 
            "text": "The Iris Dataset\n\n\nA very common dataset to test algorithms with is the \nIris Dataset\n . The following explains how to build a neural network from the command line, programmatically in java and in the Weka workbench GUI.\n\n\nThe iris dataset can be found in the \ndatasets/nominal\n directory of the WekaDeeplearning4j package.\n\n\nCommandline\n\n\nStarting simple, the most straight forward way to create a neural network with this package is by using the commandline. A Single-Layer-Perceptron (the most basic neural network possible) is shown in the following\n\n\n$ java -cp weka.jar weka.Run \\\n       .Dl4jMlpClassifier \\\n       -S 1 \\\n       -layer \nweka.dl4j.layers.OutputLayer \\\n              -activation weka.dl4j.activations.ActivationSoftmax \\\n              -updater SGD \\\n              -lr 0.01 \\\n              -name \\\nOutput layer\\\n \\\n              -lossFn weka.dl4j.lossfunctions.LossMCXENT\n \\\n       -numEpochs 10 \\\n       -t datasets/nominal/iris.arff \\\n       -split-percentage 66\n\n\n\n\nJava\n\n\nThe same architecture can be built programmatically with the following Java code\n\n\n// Create a new Multi-Layer-Perceptron classifier\nDl4jMlpClassifier clf = new Dl4jMlpClassifier();\n// Set a seed for reproducable results\nclf.setSeed(1);\n\n// Load the iris dataset and set its class index\nInstances data = new Instances(new FileReader(\ndatasets/nominal/iris.arff\n));\ndata.setClassIndex(data.numAttributes() - 1);\n\n// Define the output layer\nOutputLayer outputLayer = new OutputLayer();\noutputLayer.setActivationFn(new ActivationSoftmax());\noutputLayer.setUpdater(Updater.SGD);\noutputLayer.setLearningRate(0.01);\noutputLayer.setLossFn(new LossMCXENT());\n\n// Add the layers to the classifier\nclf.setLayers(new Layer[]{outputLayer});\n\n// Evaluate the network\nEvaluation eval = new Evaluation(data);\nint numFolds = 10;\neval.crossValidateModel(clf, data, numFolds, new Random(1));\n\nSystem.out.println(eval.toSummaryString());", 
            "title": "Classifying the Iris Dataset"
        }, 
        {
            "location": "/examples/classifying-iris/#the-iris-dataset", 
            "text": "A very common dataset to test algorithms with is the  Iris Dataset  . The following explains how to build a neural network from the command line, programmatically in java and in the Weka workbench GUI.  The iris dataset can be found in the  datasets/nominal  directory of the WekaDeeplearning4j package.", 
            "title": "The Iris Dataset"
        }, 
        {
            "location": "/examples/classifying-iris/#commandline", 
            "text": "Starting simple, the most straight forward way to create a neural network with this package is by using the commandline. A Single-Layer-Perceptron (the most basic neural network possible) is shown in the following  $ java -cp weka.jar weka.Run \\\n       .Dl4jMlpClassifier \\\n       -S 1 \\\n       -layer  weka.dl4j.layers.OutputLayer \\\n              -activation weka.dl4j.activations.ActivationSoftmax \\\n              -updater SGD \\\n              -lr 0.01 \\\n              -name \\ Output layer\\  \\\n              -lossFn weka.dl4j.lossfunctions.LossMCXENT  \\\n       -numEpochs 10 \\\n       -t datasets/nominal/iris.arff \\\n       -split-percentage 66", 
            "title": "Commandline"
        }, 
        {
            "location": "/examples/classifying-iris/#java", 
            "text": "The same architecture can be built programmatically with the following Java code  // Create a new Multi-Layer-Perceptron classifier\nDl4jMlpClassifier clf = new Dl4jMlpClassifier();\n// Set a seed for reproducable results\nclf.setSeed(1);\n\n// Load the iris dataset and set its class index\nInstances data = new Instances(new FileReader( datasets/nominal/iris.arff ));\ndata.setClassIndex(data.numAttributes() - 1);\n\n// Define the output layer\nOutputLayer outputLayer = new OutputLayer();\noutputLayer.setActivationFn(new ActivationSoftmax());\noutputLayer.setUpdater(Updater.SGD);\noutputLayer.setLearningRate(0.01);\noutputLayer.setLossFn(new LossMCXENT());\n\n// Add the layers to the classifier\nclf.setLayers(new Layer[]{outputLayer});\n\n// Evaluate the network\nEvaluation eval = new Evaluation(data);\nint numFolds = 10;\neval.crossValidateModel(clf, data, numFolds, new Random(1));\n\nSystem.out.println(eval.toSummaryString());", 
            "title": "Java"
        }, 
        {
            "location": "/examples/classifying-mnist/", 
            "text": "The MNIST Dataset\n\n\nThe MNIST dataset provides images of handwritten digits of 10 classes (0-9) and suits the task of simple image classification. \n\n\nThe minimal MNIST arff file can be found in the \ndatasets/nominal\n directory of the WekaDeeplearning4j package. This arff file lists all images in \ndatasets/nominal/mnist-minimal\n and annotates their path with their class label.\n\n\nImportant note:\n The arff dataset contains two features, the first one being the \nfilename\n and the second one being the \nclass\n. Therefore it is necessary to define an \nImageDataSetIterator\n which uses these filenames in the directory given by the option \n-imagesLocation\n.\n\n\nCommandline\n\n\nThe following run creates a Conv(3x3x8)\nPool(2x2,MAX)\nConv(3x3x8)\nPool(2x2,MAX)\nOut architecture\n\n\n$ java -Xmx5g -cp weka.jar weka.Run \\\n     .Dl4jMlpClassifier \\\n     -S 1 \\\n     -iterator \nweka.dl4j.iterators.instance.ImageInstanceIterator -imagesLocation datasets/nominal/mnist-minimal -numChannels 1 -height 28 -width 28 -bs 16\n \\\n     -normalization \nStandardize training data\n \\\n     -layer \nweka.dl4j.layers.ConvolutionLayer -nFilters 8 -activation weka.dl4j.activations.ActivationReLU -kernelSizeX 3 -kernelSizeY 3 -paddingX 0 -paddingY 0 -strideX 1 -strideY 1\n \\\n     -layer \nweka.dl4j.layers.SubsamplingLayer -kernelSizeX 2 -kernelSizeY 2 -paddingX 0 -paddingY 0 -poolingType MAX -strideX 1 -strideY 1\n \\\n     -layer \nweka.dl4j.layers.ConvolutionLayer -nFilters 8 -activation weka.dl4j.activations.ActivationReLU -kernelSizeX 3 -kernelSizeY 3 -paddingX 0 -paddingY 0 -strideX 1 -strideY 1\n \\\n     -layer \nweka.dl4j.layers.SubsamplingLayer -kernelSizeX 2 -kernelSizeY 2 -paddingX 0 -paddingY 0 -poolingType MAX -strideX 1 -strideY 1\n \\\n     -layer \nweka.dl4j.layers.OutputLayer -activation weka.dl4j.activations.ActivationSoftmax -lossFn weka.dl4j.lossfunctions.LossMCXENT -updater ADAM\n \\\n     -numEpochs 10 \\\n     -t datasets/nominal/mnist.meta.minimal.arff \\\n     -split-percentage 80\n\n\n\n\nJava\n\n\nThe same architecture can be built programmatically with the following Java code\n\n\n// Set up the MLP classifier\nDl4jMlpClassifier clf = new Dl4jMlpClassifier();\nclf.setSeed(1);\nclf.setNumEpochs(10);\n\n\n// Load the arff file\nInstances data = new Instances(new FileReader(\ndatasets/nominal/mnist.meta.minimal.arff\n));\ndata.setClassIndex(data.numAttributes() - 1);\n\n\n// Load the image iterator\nImageDataSetIterator imgIter = new ImageDataSetIterator();\nimgIter.setImagesLocation(new File(\ndatasets/nominal/mnist-minimal\n));\nimgIter.setHeight(28);\nimgIter.setWidth(28);\nimgIter.setNumChannels(1);\nimgIter.setTrainBatchSize(16);\nclf.setDataSetIterator(imgIter);\n\n\n// Setup the network layers\n// First convolution layer, 8 3x3 filter \nConvolutionLayer convLayer1 = new ConvolutionLayer();\nconvLayer1.setKernelSizeX(3);\nconvLayer1.setKernelSizeY(3);\nconvLayer1.setStrideX(1);\nconvLayer1.setStrideY(1);\nconvLayer1.setActivationFn(new ActivationReLU());\nconvLayer1.setNOut(8);\n\n// First maxpooling layer, 2x2 filter\nSubsamplingLayer poolLayer1 = new SubsamplingLayer();\npoolLayer1.setPoolingType(PoolingType.MAX);\npoolLayer1.setKernelSizeX(2);\npoolLayer1.setKernelSizeY(2);\npoolLayer1.setStrideX(1);\npoolLayer1.setStrideY(1);\n\n// Second convolution layer, 8 3x3 filter\nConvolutionLayer convLayer2 = new ConvolutionLayer();\nconvLayer2.setKernelSizeX(3);\nconvLayer2.setKernelSizeY(3);\nconvLayer2.setStrideX(1);\nconvLayer2.setStrideY(1);\nconvLayer2.setActivationFn(new ActivationReLU());\nconvLayer2.setNOut(8);\n\n// Second maxpooling layer, 2x2 filter\nSubsamplingLayer poolLayer2 = new SubsamplingLayer();\npoolLayer2.setPoolingType(PoolingType.MAX);\npoolLayer2.setKernelSizeX(2);\npoolLayer2.setKernelSizeY(2);\npoolLayer2.setStrideX(1);\npoolLayer2.setStrideY(1);\n\n// Output layer with softmax activation\nOutputLayer outputLayer = new OutputLayer();\noutputLayer.setActivationFn(new ActivationSoftmax());\noutputLayer.setLossFn(new LossMCXENT());\n\n\n// Set up the network configuration\nNeuralNetConfiguration nnc = new NeuralNetConfiguration();\nnnc.setOptimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT);\nclf.setNeuralNetConfiguration(nnc);\n\n\n// Set the layers\nclf.setLayers(new Layer[]{convLayer1, poolLayer1, convLayer2, poolLayer2, denseLayer, outputLayer});\n\n\n// Evaluate the network\nEvaluation eval = new Evaluation(data);\nint numFolds = 10;\neval.crossValidateModel(clf, data, numFolds, new Random(1));\n\nSystem.out.println(\n% Correct = \n + eval.pctCorrect());", 
            "title": "Classifying the MNIST Dataset"
        }, 
        {
            "location": "/examples/classifying-mnist/#the-mnist-dataset", 
            "text": "The MNIST dataset provides images of handwritten digits of 10 classes (0-9) and suits the task of simple image classification.   The minimal MNIST arff file can be found in the  datasets/nominal  directory of the WekaDeeplearning4j package. This arff file lists all images in  datasets/nominal/mnist-minimal  and annotates their path with their class label.  Important note:  The arff dataset contains two features, the first one being the  filename  and the second one being the  class . Therefore it is necessary to define an  ImageDataSetIterator  which uses these filenames in the directory given by the option  -imagesLocation .", 
            "title": "The MNIST Dataset"
        }, 
        {
            "location": "/examples/classifying-mnist/#commandline", 
            "text": "The following run creates a Conv(3x3x8) Pool(2x2,MAX) Conv(3x3x8) Pool(2x2,MAX) Out architecture  $ java -Xmx5g -cp weka.jar weka.Run \\\n     .Dl4jMlpClassifier \\\n     -S 1 \\\n     -iterator  weka.dl4j.iterators.instance.ImageInstanceIterator -imagesLocation datasets/nominal/mnist-minimal -numChannels 1 -height 28 -width 28 -bs 16  \\\n     -normalization  Standardize training data  \\\n     -layer  weka.dl4j.layers.ConvolutionLayer -nFilters 8 -activation weka.dl4j.activations.ActivationReLU -kernelSizeX 3 -kernelSizeY 3 -paddingX 0 -paddingY 0 -strideX 1 -strideY 1  \\\n     -layer  weka.dl4j.layers.SubsamplingLayer -kernelSizeX 2 -kernelSizeY 2 -paddingX 0 -paddingY 0 -poolingType MAX -strideX 1 -strideY 1  \\\n     -layer  weka.dl4j.layers.ConvolutionLayer -nFilters 8 -activation weka.dl4j.activations.ActivationReLU -kernelSizeX 3 -kernelSizeY 3 -paddingX 0 -paddingY 0 -strideX 1 -strideY 1  \\\n     -layer  weka.dl4j.layers.SubsamplingLayer -kernelSizeX 2 -kernelSizeY 2 -paddingX 0 -paddingY 0 -poolingType MAX -strideX 1 -strideY 1  \\\n     -layer  weka.dl4j.layers.OutputLayer -activation weka.dl4j.activations.ActivationSoftmax -lossFn weka.dl4j.lossfunctions.LossMCXENT -updater ADAM  \\\n     -numEpochs 10 \\\n     -t datasets/nominal/mnist.meta.minimal.arff \\\n     -split-percentage 80", 
            "title": "Commandline"
        }, 
        {
            "location": "/examples/classifying-mnist/#java", 
            "text": "The same architecture can be built programmatically with the following Java code  // Set up the MLP classifier\nDl4jMlpClassifier clf = new Dl4jMlpClassifier();\nclf.setSeed(1);\nclf.setNumEpochs(10);\n\n\n// Load the arff file\nInstances data = new Instances(new FileReader( datasets/nominal/mnist.meta.minimal.arff ));\ndata.setClassIndex(data.numAttributes() - 1);\n\n\n// Load the image iterator\nImageDataSetIterator imgIter = new ImageDataSetIterator();\nimgIter.setImagesLocation(new File( datasets/nominal/mnist-minimal ));\nimgIter.setHeight(28);\nimgIter.setWidth(28);\nimgIter.setNumChannels(1);\nimgIter.setTrainBatchSize(16);\nclf.setDataSetIterator(imgIter);\n\n\n// Setup the network layers\n// First convolution layer, 8 3x3 filter \nConvolutionLayer convLayer1 = new ConvolutionLayer();\nconvLayer1.setKernelSizeX(3);\nconvLayer1.setKernelSizeY(3);\nconvLayer1.setStrideX(1);\nconvLayer1.setStrideY(1);\nconvLayer1.setActivationFn(new ActivationReLU());\nconvLayer1.setNOut(8);\n\n// First maxpooling layer, 2x2 filter\nSubsamplingLayer poolLayer1 = new SubsamplingLayer();\npoolLayer1.setPoolingType(PoolingType.MAX);\npoolLayer1.setKernelSizeX(2);\npoolLayer1.setKernelSizeY(2);\npoolLayer1.setStrideX(1);\npoolLayer1.setStrideY(1);\n\n// Second convolution layer, 8 3x3 filter\nConvolutionLayer convLayer2 = new ConvolutionLayer();\nconvLayer2.setKernelSizeX(3);\nconvLayer2.setKernelSizeY(3);\nconvLayer2.setStrideX(1);\nconvLayer2.setStrideY(1);\nconvLayer2.setActivationFn(new ActivationReLU());\nconvLayer2.setNOut(8);\n\n// Second maxpooling layer, 2x2 filter\nSubsamplingLayer poolLayer2 = new SubsamplingLayer();\npoolLayer2.setPoolingType(PoolingType.MAX);\npoolLayer2.setKernelSizeX(2);\npoolLayer2.setKernelSizeY(2);\npoolLayer2.setStrideX(1);\npoolLayer2.setStrideY(1);\n\n// Output layer with softmax activation\nOutputLayer outputLayer = new OutputLayer();\noutputLayer.setActivationFn(new ActivationSoftmax());\noutputLayer.setLossFn(new LossMCXENT());\n\n\n// Set up the network configuration\nNeuralNetConfiguration nnc = new NeuralNetConfiguration();\nnnc.setOptimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT);\nclf.setNeuralNetConfiguration(nnc);\n\n\n// Set the layers\nclf.setLayers(new Layer[]{convLayer1, poolLayer1, convLayer2, poolLayer2, denseLayer, outputLayer});\n\n\n// Evaluate the network\nEvaluation eval = new Evaluation(data);\nint numFolds = 10;\neval.crossValidateModel(clf, data, numFolds, new Random(1));\n\nSystem.out.println( % Correct =   + eval.pctCorrect());", 
            "title": "Java"
        }, 
        {
            "location": "/troubleshooting/", 
            "text": "This section shall provide solutions for issues that  may appear.\n\n\n\n\nCUDA: GOMP Version 4.0 not found\n\n\nIssue\n\n\nStarting the \nDl4jMlpClassifier\n while using the GPU version of the package results in something similar to:\n\n\nCaused by: java.lang.UnsatisfiedLinkError: \n    /home/user/.javacpp/cache/nd4j-cuda-8.0-0.9.1-linux-x86_64.jar/org/nd4j/nativeblas/linux-x86_64/libjnind4jcuda.so: \n    /usr/lib/x86_64-linux-gnu/libgomp.so.1: \n    version `GOMP_4.0' not found \n    (required by /home/user/.javacpp/cache/nd4j-cuda-8.0-0.9.1-linux-x86_64.jar/org/nd4j/nativeblas/linux-x86_64/libnd4jcuda.so)\n\n\n\n\n\nThis happens when your system is using a version below 4.9 of the gcc compiler. You can check this with:\n\n\n$ gcc --version\ngcc (Ubuntu 4.8.4-2ubuntu1~14.04.3) 4.8.4\nCopyright (C) 2013 Free Software Foundation, Inc.\nThis is free software; see the source for copying conditions.  There is NO\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n\n\n\n\nTherefore the libgomp.so.1 library is still of version 3.0, while the nd4j backend expects version 4.0.\n\n\nSolution\n\n\nDownload the latest version of libgomp for your system and export the following:\n\n\nexport LD_PRELOAD=\nPATH-TO-NEW-LIBGOMP.SO\n\n\n\n\n\nFor Ubuntu you can get the library \nhere\n, choose your architecture, download and extract the deb-file. For amd64 architectures this would be:\n\n\n$ wget http://security.ubuntu.com/ubuntu/pool/main/g/gcc-5/libgomp1_5.4.0-6ubuntu1~16.04.4_amd64.deb\n$ ar vx libgomp1_5.4.0-6ubuntu1~16.04.4_amd64.deb\n$ tar -xvf data.tar.xz\n\n\n\n\nThis extracts the library to \n./usr/lib/x86_64-linux-gnu/libgomp.so.1\n. Afterward set the \nLD_PRELOAD\n variable to this path as an absolute path and export it as shown above.\n\n\n\n\nCUDA: Failed to allocate X bytes from DEVICE memory\n\n\nIssue\n\n\nYour network architecture or your batch size consumes too much memory.\n\n\nSolution\n\n\nUse a lower batch size, or adjust your Java heap and off-heap limits to your available memory accordingly to the \nofficial Dl4J memory description\n.", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/troubleshooting/#cuda-gomp-version-40-not-found", 
            "text": "", 
            "title": "CUDA: GOMP Version 4.0 not found"
        }, 
        {
            "location": "/troubleshooting/#issue", 
            "text": "Starting the  Dl4jMlpClassifier  while using the GPU version of the package results in something similar to:  Caused by: java.lang.UnsatisfiedLinkError: \n    /home/user/.javacpp/cache/nd4j-cuda-8.0-0.9.1-linux-x86_64.jar/org/nd4j/nativeblas/linux-x86_64/libjnind4jcuda.so: \n    /usr/lib/x86_64-linux-gnu/libgomp.so.1: \n    version `GOMP_4.0' not found \n    (required by /home/user/.javacpp/cache/nd4j-cuda-8.0-0.9.1-linux-x86_64.jar/org/nd4j/nativeblas/linux-x86_64/libnd4jcuda.so)  This happens when your system is using a version below 4.9 of the gcc compiler. You can check this with:  $ gcc --version\ngcc (Ubuntu 4.8.4-2ubuntu1~14.04.3) 4.8.4\nCopyright (C) 2013 Free Software Foundation, Inc.\nThis is free software; see the source for copying conditions.  There is NO\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  Therefore the libgomp.so.1 library is still of version 3.0, while the nd4j backend expects version 4.0.", 
            "title": "Issue"
        }, 
        {
            "location": "/troubleshooting/#solution", 
            "text": "Download the latest version of libgomp for your system and export the following:  export LD_PRELOAD= PATH-TO-NEW-LIBGOMP.SO   For Ubuntu you can get the library  here , choose your architecture, download and extract the deb-file. For amd64 architectures this would be:  $ wget http://security.ubuntu.com/ubuntu/pool/main/g/gcc-5/libgomp1_5.4.0-6ubuntu1~16.04.4_amd64.deb\n$ ar vx libgomp1_5.4.0-6ubuntu1~16.04.4_amd64.deb\n$ tar -xvf data.tar.xz  This extracts the library to  ./usr/lib/x86_64-linux-gnu/libgomp.so.1 . Afterward set the  LD_PRELOAD  variable to this path as an absolute path and export it as shown above.", 
            "title": "Solution"
        }, 
        {
            "location": "/troubleshooting/#cuda-failed-to-allocate-x-bytes-from-device-memory", 
            "text": "", 
            "title": "CUDA: Failed to allocate X bytes from DEVICE memory"
        }, 
        {
            "location": "/troubleshooting/#issue_1", 
            "text": "Your network architecture or your batch size consumes too much memory.", 
            "title": "Issue"
        }, 
        {
            "location": "/troubleshooting/#solution_1", 
            "text": "Use a lower batch size, or adjust your Java heap and off-heap limits to your available memory accordingly to the  official Dl4J memory description .", 
            "title": "Solution"
        }
    ]
}
{
    "docs": [
        {
            "location": "/", 
            "text": "WekaDeeplearning4J: Deep Learning using Weka\n\n\n\nWekaDeeplearning4J is a deep learning package for the \nWeka\n workbench. It is developed to incorporate the modern techniques of deep learning into Weka. The backend is provided by the \nDeeplearning4J\n Java library. \n\n\nThe source code for this package is available on \nGitHub\n. The java-doc can be found \nhere\n.\n\n\nThis documentation is still work in progress.\n\n\nFunctionality\n\n\nAll functionality of this package is accessible via the Weka GUI, the commandline and programmatically in Java.\n\n\nThe following Neural Network Layers are available to build sophisticated architectures:\n\n\n\n\nConvolutionLayer\n: applying convolution, useful for images and text embeddings\n\n\nDenseLayer\n: all units are connected to all units of its parent layer\n\n\nSubsamplingLayer\n: subsample from groups of units of the parent layer by different strategies (average, maximum, etc.)\n\n\nBatchNormalization\n: applies the common batch normalization strategy on the activations of the parent layer\n\n\nLSTM\n: uses long short term memory approach\n\n\nOutputLayer\n: generates classification / regression outputs\n\n\n\n\nFurther configurations can be found in the \nGetting Started\n and the \nExamples\n sections.", 
            "title": "Home"
        }, 
        {
            "location": "/#wekadeeplearning4j-deep-learning-using-weka", 
            "text": "WekaDeeplearning4J is a deep learning package for the  Weka  workbench. It is developed to incorporate the modern techniques of deep learning into Weka. The backend is provided by the  Deeplearning4J  Java library.   The source code for this package is available on  GitHub . The java-doc can be found  here .  This documentation is still work in progress.", 
            "title": "WekaDeeplearning4J: Deep Learning using Weka"
        }, 
        {
            "location": "/#functionality", 
            "text": "All functionality of this package is accessible via the Weka GUI, the commandline and programmatically in Java.  The following Neural Network Layers are available to build sophisticated architectures:   ConvolutionLayer : applying convolution, useful for images and text embeddings  DenseLayer : all units are connected to all units of its parent layer  SubsamplingLayer : subsample from groups of units of the parent layer by different strategies (average, maximum, etc.)  BatchNormalization : applies the common batch normalization strategy on the activations of the parent layer  LSTM : uses long short term memory approach  OutputLayer : generates classification / regression outputs   Further configurations can be found in the  Getting Started  and the  Examples  sections.", 
            "title": "Functionality"
        }, 
        {
            "location": "/install/", 
            "text": "Prerequisites\n\n\n\n\nWeka 3.8.0 or above (\nhere\n)\n\n\nWekaDeeplearning4j package 1.3.4 or above (\nhere\n)\n\n\n\n\nYou need to unzip the Weka zip file to a directory of your choice.\n\n\nCPU\n\n\nFor the CPU package no further requisites are necessary.\n\n\nGPU\n\n\nThe GPU package needs the CUDA 8.0 backend to be installed on your system. Nvidia provides some good installation instructions for all platforms:\n\n\n\n\nLinux\n\n\nMac OS X\n\n\nWindows\n\n\n\n\nInstalling the Weka Package\n\n\nWeka packages can be easily installed either via the user interface as described \nhere\n, or simply via the commandline:\n\n\n$ java -cp \nWEKA-JAR-PATH\n weka.core.WekaPackageManager \\\n       -install-package wekaDeeplearning4j-\nBACKEND\n-\nPLATFORM\n.zip\n\n\n\n\nwhere \nWEKA-JAR-PATH\n must be replaced by the path pointing to the Weka jar file, \nBACKEND\n must be replaced by either \nCPU\n or \nGPU\n, depending on which version you chose and \nPLATFORM\n must be replaced with your operating system (linux, macosx, windows).\n\n\nYou can check whether the installation was successful with\n\n\n$ java -cp \nWEKA-JAR-PATH\n weka.core.WekaPackageManager \\\n       -list-packages installed\n\n\n\n\nwhich results in\n\n\nInstalled   Repository  Loaded  Package\n=========   ==========  ======  =======\n1.3.4       -----       Yes     wekaDeeplearning4j-\nBACKEND\n-\nPLATFORM\n: Weka wrappers for Deeplearning4j\n\n\n\n\nUsing wekaDeeplearning4j in a Maven Project\n\n\nIt is also possible to include this package as maven project. As of now it is not provided in any maven repository, therefore you need to install this package to your local \n.m2\n repository:\n\n\n$ git clone https://github.com/Waikato/wekaDeeplearning4j.git\n$ cd wekaDeeplearning4j/package\n$ mvn clean install -P \nbackend\n # Replace \nbackend\n with either \nCPU\n or \nGPU\n\n\n\n\n\nNow you can add the maven dependency in your \npom.xml\n file \n\n\ndependency\n\n    \ngroupId\nnz.ac.waikato.cms.weka\n/groupId\n\n    \nartifactId\nwekaDeeplearning4j\n/artifactId\n\n    \nversion\n${wekaDeeplearning4j.version}\n/version\n\n\n/dependency\n\n\n\n\n\nWhen using the CPU the following two dependencies have to be added:\n\n\n!--CPU Specific--\n\n\ndependency\n\n    \ngroupId\norg.nd4j\n/groupId\n\n    \nartifactId\nnd4j-native-platform\n/artifactId\n\n    \nversion\n${nd4j.version}\n/version\n\n\n/dependency\n\n\ndependency\n\n    \ngroupId\norg.bytedeco.javacpp-presets\n/groupId\n\n    \nartifactId\nopenblas-platform\n/artifactId\n\n    \nversion\n0.2.19-1.3\n/version\n\n\n/dependency\n\n\n\n\n\nwhile using the GPU requires to add nd4j GPU dependencies:\n\n\n!--GPU Specific--\n\n\ndependency\n\n    \ngroupId\norg.bytedeco.javacpp-presets\n/groupId\n\n    \nartifactId\ncuda\n/artifactId\n\n    \nversion\n8.0-6.0-1.3\n/version\n\n\n/dependency\n\n\ndependency\n\n    \ngroupId\norg.nd4j\n/groupId\n\n    \nartifactId\nnd4j-cuda-8.0-platform\n/artifactId\n\n    \nversion\n${nd4j.version}\n/version\n\n\n/dependency", 
            "title": "Installation"
        }, 
        {
            "location": "/install/#prerequisites", 
            "text": "Weka 3.8.0 or above ( here )  WekaDeeplearning4j package 1.3.4 or above ( here )   You need to unzip the Weka zip file to a directory of your choice.", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/install/#cpu", 
            "text": "For the CPU package no further requisites are necessary.", 
            "title": "CPU"
        }, 
        {
            "location": "/install/#gpu", 
            "text": "The GPU package needs the CUDA 8.0 backend to be installed on your system. Nvidia provides some good installation instructions for all platforms:   Linux  Mac OS X  Windows", 
            "title": "GPU"
        }, 
        {
            "location": "/install/#installing-the-weka-package", 
            "text": "Weka packages can be easily installed either via the user interface as described  here , or simply via the commandline:  $ java -cp  WEKA-JAR-PATH  weka.core.WekaPackageManager \\\n       -install-package wekaDeeplearning4j- BACKEND - PLATFORM .zip  where  WEKA-JAR-PATH  must be replaced by the path pointing to the Weka jar file,  BACKEND  must be replaced by either  CPU  or  GPU , depending on which version you chose and  PLATFORM  must be replaced with your operating system (linux, macosx, windows).  You can check whether the installation was successful with  $ java -cp  WEKA-JAR-PATH  weka.core.WekaPackageManager \\\n       -list-packages installed  which results in  Installed   Repository  Loaded  Package\n=========   ==========  ======  =======\n1.3.4       -----       Yes     wekaDeeplearning4j- BACKEND - PLATFORM : Weka wrappers for Deeplearning4j", 
            "title": "Installing the Weka Package"
        }, 
        {
            "location": "/install/#using-wekadeeplearning4j-in-a-maven-project", 
            "text": "It is also possible to include this package as maven project. As of now it is not provided in any maven repository, therefore you need to install this package to your local  .m2  repository:  $ git clone https://github.com/Waikato/wekaDeeplearning4j.git\n$ cd wekaDeeplearning4j/package\n$ mvn clean install -P  backend  # Replace  backend  with either  CPU  or  GPU   Now you can add the maven dependency in your  pom.xml  file   dependency \n     groupId nz.ac.waikato.cms.weka /groupId \n     artifactId wekaDeeplearning4j /artifactId \n     version ${wekaDeeplearning4j.version} /version  /dependency   When using the CPU the following two dependencies have to be added:  !--CPU Specific--  dependency \n     groupId org.nd4j /groupId \n     artifactId nd4j-native-platform /artifactId \n     version ${nd4j.version} /version  /dependency  dependency \n     groupId org.bytedeco.javacpp-presets /groupId \n     artifactId openblas-platform /artifactId \n     version 0.2.19-1.3 /version  /dependency   while using the GPU requires to add nd4j GPU dependencies:  !--GPU Specific--  dependency \n     groupId org.bytedeco.javacpp-presets /groupId \n     artifactId cuda /artifactId \n     version 8.0-6.0-1.3 /version  /dependency  dependency \n     groupId org.nd4j /groupId \n     artifactId nd4j-cuda-8.0-platform /artifactId \n     version ${nd4j.version} /version  /dependency", 
            "title": "Using wekaDeeplearning4j in a Maven Project"
        }, 
        {
            "location": "/user-guide/getting-started/", 
            "text": "Usage\n\n\nIf you are new to Weka, you should probably first start reading the \nWeka primer\n as a basic introduction.\n\n\nAs most of Weka, the WekaDeeplearning4j's functionality is accessible in three ways:\n\n\n\n\nVia the commandline interface\n\n\nProgramming with Weka in Java\n\n\nUsing the Weka workbench GUI\n\n\n\n\nAll three ways are explained in the following. The main classifier exposed by this package is named \nDl4jMlpClassifier\n.\nSimple examples are given in the examples section for the \nIris dataset\n and the \nMNIST dataset\n.\n\n\nMake sure your \nWEKA_HOME\n environment variable is set.\n\n\nCommandline Interface\n\n\nA first look for the available commandline options of the \nDl4jMlpClassifier\n is shown with\n\n\n$ java -cp $WEKA_HOME/weka.jar weka.Run .Dl4jMlpClassifier -h\n\n\n\n\nBelow the general options, the specific ones are listed:\n\n\nOptions specific to weka.classifiers.functions.Dl4jMlpClassifier:\n\n-S \nnum\n\n    Random number seed.\n    (default 1)\n-logFile \nstring\n\n    The name of the log file to write loss information to (default = no log file).\n-layer \nstring\n\n    The specification of a layer. This option can be used multiple times.\n-numEpochs \nint\n\n    The number of epochs to perform.\n-iterator \nstring\n\n    The dataset iterator to use.\n-config \nstring\n\n    The neural network configuration to use.\n-normalization \nint\n\n    The type of normalization to perform.\n-queueSize \nint\n\n    The queue size for asynchronous data transfer (default: 0, synchronous transfer).\n-output-debug-info\n    If set, classifier is run in debug mode and\n    may output additional info to the console\n-do-not-check-capabilities\n    If set, classifier capabilities are not checked before classifier is built\n    (use with caution).\n-num-decimal-places\n    The number of decimal places for the output of numbers in the model (default 2).\n-batch-size\n    The desired batch size for batch prediction  (default 100).\n\n\n\n\nThe most interesting option may be the \n-layer\n specification. This option can be used multiple times and defines the architecture of the network layer-wise. \n\n\n$ java -cp $WEKA_HOME/weka.jar weka.Run \\\n       .Dl4jMlpClassifier \\\n       -layer \nweka.dl4j.layers.DenseLayer \\\n              -activation weka.dl4j.activations.ActivationReLU \\\n              -nOut 10\n \\\n       -layer \nweka.dl4j.layers.OutputLayer \\\n              -activation weka.dl4j.activations.ActivationSoftmax \\\n              -lossFn weka.dl4j.lossfunctions.LossMCXENT\n \n\n\n\n\nThe above setup builds a network with one hidden layer, having 10 output units using the ReLU activation function, followed by an output layer with the softmax activation function, using a multi-class cross-entropy loss function (MCXENT) as optimization objective.\n\n\nAnother important option is the neural network configuration \n-conf\n in which you can setup hyperparameters for the network. Available options can be found in the \nJava documentation\n (the field \ncommandLineParamSynopsis\n indicates the commandline parameter name for each available method).\n\n\nJava\n\n\nThe Java API is a straight forward wrapper for the official DeepLearning4j API. Using the \nDl4jMlPClassifier\n your code should usually start with\n\n\n// Create a new Multi-Layer-Perceptron classifier\nDl4jMlpClassifier clf = new Dl4jMlpClassifier();\n\n\n\n\nThe networks architecture can be set up by creating each layer step by step:\n\n\nDenseLayer denseLayer = new DenseLayer();\ndenseLayer.setNOut(10);\ndenseLayer.setActivationFn(new ActivationReLU());\n\n// Define the output layer\nOutputLayer outputLayer = new OutputLayer();\noutputLayer.setActivationFn(new ActivationSoftmax());\n\n\n\n\nFurther configuration can be done by setting a \nNeuralNetConfiguration\n\n\nNeuralNetConfiguration nnc = new NeuralNetConfiguration();\nnnc.setUpdater(new Adam());\nclf.setNeuralNetConfiguration(nnc);\n\n\n\n\nFinally the layers are set with\n\n\n// Add the layers to the classifier\nclf.setLayers(new Layer[]{denseLayer, outputLayer});\n\n\n\n\nWithout Maven\n\n\nIf you are not using the package in a maven project as described \nhere\n, you need to add the following directories to your java classpath\n\n\n\n\n$WEKA_HOME/packages/wekaDeeplearning4j-\nBACKEND\n-\nPLATFORM\n/*\n\n\n$WEKA_HOME/packages/wekaDeeplearning4j-\nBACKEND\n-\nPLATFORM\n/lib*\n\n\n\n\nAssuming you have the following \nMain.java\n file:\n\n\nimport weka.classifiers.Evaluation;\nimport weka.classifiers.functions.Dl4jMlpClassifier;\nimport weka.core.Instances;\n\nimport java.io.FileReader;\nimport java.nio.file.Paths;\nimport java.util.Random;\n\npublic class Main {\n    public static void main(String[] args) throws Exception {\n        Dl4jMlpClassifier clf = new Dl4jMlpClassifier();\n        String irisPath = Paths.get(System.getenv(\nWEKA_HOME\n), \npackages\n, \nwekaDeeplearning4j-\nBACKEND\n-\nPLATFORM\n, \ndatasets\n, \nnominal\n, \niris.arff\n).toString();\n        Instances inst = new Instances(new FileReader(irisPath));\n        inst.setClassIndex(inst.numAttributes() - 1);\n        Evaluation ev = new Evaluation(inst);\n        ev.crossValidateModel(clf, inst, 10, new Random(0));\n        System.out.println(ev.toSummaryString());\n    }\n}\n\n\n\n\nYou can now compile it including the libraries in the classpath:\n\n\njavac -cp \n$WEKA_HOME/weka.jar:$WEKA_HOME/packages/wekaDeeplearning4j-\nBACKEND\n-\nPLATFORM\n/*:$WEKA_HOME/packages/wekaDeeplearning4j-\nBACKEND\n-\nPLATFORM\n/lib/*\n Main.java\n\n\n\n\nand run it with:\n\n\njava -cp \n$WEKA_HOME/weka.jar:$WEKA_HOME/packages/wekaDeeplearning4j\nBACKEND\n-\nPLATFORM\n/*:$WEKA_HOME/packages/wekaDeeplearning4j\nBACKEND\n-\nPLATFORM\n/lib/*:.\n Main\n\n\n\n\n(Use \n;\n as classpath separator for Windows instead) \n\n\nGUI\n\n\nA tutorial on how to use the GUI is coming soon.\n\n\nModel Zoo\n\n\nWekaDeeplearning4j adapts the model zoo of Deeplearning4j. That means it is possible to load predefined architectures as neural network and train it on a new dataset. Currently implemented architectures are:\n\n\n\n\nAlexNet\n\n\nLeNet\n\n\nSimpleCNN\n\n\nVGG16\n\n\nVGG19\n\n\nResNet50\n\n\nInceptionResNetV1\n\n\n\n\nThis set of models will be extended over the time.\n\n\nTo set a predefined model, e.g. LeNet, from the model zoo, it is necessary to add the \n-zooModel \"weka.dl4j.zoo.LeNet\"\n option via commandline, or call the \nsetZooModel(new LeNet())\n on the \nDl4jMlpClassifier\n object.\n\n\nEarly Stopping\n\n\nEarly stopping allows to stop the training process as soon as the network does not improve its loss on a validation set for \nN\n epochs. \n\n\nThe setup below adds an early stopping condition that checks whether the loss score on 10% of the training data did not improve successively for 5 epochs.\n\n\nCommandline\n\n\n-early-stopping \nweka.dl4j.earlystopping.EarlyStopping -maxEpochsNoImprovement 5 -valPercentage 10\n\n\n\n\n\nJava\n\n\nEarlyStopping es = new EarlyStopping();\nes.setMaxEpochsNoImprovement(5);\nes.setValidationSetPercentage(10);\nclf.setEarlyStopping(es)\n\n\n\n\nor simpler\n\n\nclf.setEarlyStopping(new EarlyStopping(5, 10))\n\n\n\n\nGUI\n\n\nThe GUI provides a simple and intuitive interface to configure the early stopping parameters:", 
            "title": "Getting Started"
        }, 
        {
            "location": "/user-guide/getting-started/#usage", 
            "text": "If you are new to Weka, you should probably first start reading the  Weka primer  as a basic introduction.  As most of Weka, the WekaDeeplearning4j's functionality is accessible in three ways:   Via the commandline interface  Programming with Weka in Java  Using the Weka workbench GUI   All three ways are explained in the following. The main classifier exposed by this package is named  Dl4jMlpClassifier .\nSimple examples are given in the examples section for the  Iris dataset  and the  MNIST dataset .  Make sure your  WEKA_HOME  environment variable is set.", 
            "title": "Usage"
        }, 
        {
            "location": "/user-guide/getting-started/#commandline-interface", 
            "text": "A first look for the available commandline options of the  Dl4jMlpClassifier  is shown with  $ java -cp $WEKA_HOME/weka.jar weka.Run .Dl4jMlpClassifier -h  Below the general options, the specific ones are listed:  Options specific to weka.classifiers.functions.Dl4jMlpClassifier:\n\n-S  num \n    Random number seed.\n    (default 1)\n-logFile  string \n    The name of the log file to write loss information to (default = no log file).\n-layer  string \n    The specification of a layer. This option can be used multiple times.\n-numEpochs  int \n    The number of epochs to perform.\n-iterator  string \n    The dataset iterator to use.\n-config  string \n    The neural network configuration to use.\n-normalization  int \n    The type of normalization to perform.\n-queueSize  int \n    The queue size for asynchronous data transfer (default: 0, synchronous transfer).\n-output-debug-info\n    If set, classifier is run in debug mode and\n    may output additional info to the console\n-do-not-check-capabilities\n    If set, classifier capabilities are not checked before classifier is built\n    (use with caution).\n-num-decimal-places\n    The number of decimal places for the output of numbers in the model (default 2).\n-batch-size\n    The desired batch size for batch prediction  (default 100).  The most interesting option may be the  -layer  specification. This option can be used multiple times and defines the architecture of the network layer-wise.   $ java -cp $WEKA_HOME/weka.jar weka.Run \\\n       .Dl4jMlpClassifier \\\n       -layer  weka.dl4j.layers.DenseLayer \\\n              -activation weka.dl4j.activations.ActivationReLU \\\n              -nOut 10  \\\n       -layer  weka.dl4j.layers.OutputLayer \\\n              -activation weka.dl4j.activations.ActivationSoftmax \\\n              -lossFn weka.dl4j.lossfunctions.LossMCXENT    The above setup builds a network with one hidden layer, having 10 output units using the ReLU activation function, followed by an output layer with the softmax activation function, using a multi-class cross-entropy loss function (MCXENT) as optimization objective.  Another important option is the neural network configuration  -conf  in which you can setup hyperparameters for the network. Available options can be found in the  Java documentation  (the field  commandLineParamSynopsis  indicates the commandline parameter name for each available method).", 
            "title": "Commandline Interface"
        }, 
        {
            "location": "/user-guide/getting-started/#java", 
            "text": "The Java API is a straight forward wrapper for the official DeepLearning4j API. Using the  Dl4jMlPClassifier  your code should usually start with  // Create a new Multi-Layer-Perceptron classifier\nDl4jMlpClassifier clf = new Dl4jMlpClassifier();  The networks architecture can be set up by creating each layer step by step:  DenseLayer denseLayer = new DenseLayer();\ndenseLayer.setNOut(10);\ndenseLayer.setActivationFn(new ActivationReLU());\n\n// Define the output layer\nOutputLayer outputLayer = new OutputLayer();\noutputLayer.setActivationFn(new ActivationSoftmax());  Further configuration can be done by setting a  NeuralNetConfiguration  NeuralNetConfiguration nnc = new NeuralNetConfiguration();\nnnc.setUpdater(new Adam());\nclf.setNeuralNetConfiguration(nnc);  Finally the layers are set with  // Add the layers to the classifier\nclf.setLayers(new Layer[]{denseLayer, outputLayer});", 
            "title": "Java"
        }, 
        {
            "location": "/user-guide/getting-started/#without-maven", 
            "text": "If you are not using the package in a maven project as described  here , you need to add the following directories to your java classpath   $WEKA_HOME/packages/wekaDeeplearning4j- BACKEND - PLATFORM /*  $WEKA_HOME/packages/wekaDeeplearning4j- BACKEND - PLATFORM /lib*   Assuming you have the following  Main.java  file:  import weka.classifiers.Evaluation;\nimport weka.classifiers.functions.Dl4jMlpClassifier;\nimport weka.core.Instances;\n\nimport java.io.FileReader;\nimport java.nio.file.Paths;\nimport java.util.Random;\n\npublic class Main {\n    public static void main(String[] args) throws Exception {\n        Dl4jMlpClassifier clf = new Dl4jMlpClassifier();\n        String irisPath = Paths.get(System.getenv( WEKA_HOME ),  packages ,  wekaDeeplearning4j- BACKEND - PLATFORM ,  datasets ,  nominal ,  iris.arff ).toString();\n        Instances inst = new Instances(new FileReader(irisPath));\n        inst.setClassIndex(inst.numAttributes() - 1);\n        Evaluation ev = new Evaluation(inst);\n        ev.crossValidateModel(clf, inst, 10, new Random(0));\n        System.out.println(ev.toSummaryString());\n    }\n}  You can now compile it including the libraries in the classpath:  javac -cp  $WEKA_HOME/weka.jar:$WEKA_HOME/packages/wekaDeeplearning4j- BACKEND - PLATFORM /*:$WEKA_HOME/packages/wekaDeeplearning4j- BACKEND - PLATFORM /lib/*  Main.java  and run it with:  java -cp  $WEKA_HOME/weka.jar:$WEKA_HOME/packages/wekaDeeplearning4j BACKEND - PLATFORM /*:$WEKA_HOME/packages/wekaDeeplearning4j BACKEND - PLATFORM /lib/*:.  Main  (Use  ;  as classpath separator for Windows instead)", 
            "title": "Without Maven"
        }, 
        {
            "location": "/user-guide/getting-started/#gui", 
            "text": "A tutorial on how to use the GUI is coming soon.", 
            "title": "GUI"
        }, 
        {
            "location": "/user-guide/getting-started/#model-zoo", 
            "text": "WekaDeeplearning4j adapts the model zoo of Deeplearning4j. That means it is possible to load predefined architectures as neural network and train it on a new dataset. Currently implemented architectures are:   AlexNet  LeNet  SimpleCNN  VGG16  VGG19  ResNet50  InceptionResNetV1   This set of models will be extended over the time.  To set a predefined model, e.g. LeNet, from the model zoo, it is necessary to add the  -zooModel \"weka.dl4j.zoo.LeNet\"  option via commandline, or call the  setZooModel(new LeNet())  on the  Dl4jMlpClassifier  object.", 
            "title": "Model Zoo"
        }, 
        {
            "location": "/user-guide/getting-started/#early-stopping", 
            "text": "Early stopping allows to stop the training process as soon as the network does not improve its loss on a validation set for  N  epochs.   The setup below adds an early stopping condition that checks whether the loss score on 10% of the training data did not improve successively for 5 epochs.  Commandline  -early-stopping  weka.dl4j.earlystopping.EarlyStopping -maxEpochsNoImprovement 5 -valPercentage 10   Java  EarlyStopping es = new EarlyStopping();\nes.setMaxEpochsNoImprovement(5);\nes.setValidationSetPercentage(10);\nclf.setEarlyStopping(es)  or simpler  clf.setEarlyStopping(new EarlyStopping(5, 10))  GUI  The GUI provides a simple and intuitive interface to configure the early stopping parameters:", 
            "title": "Early Stopping"
        }, 
        {
            "location": "/user-guide/data/", 
            "text": "Loading Data\n\n\nThe package provides so called \nInstanceIterators\n to load the given dataset in a correct shape. Each iterator allows to set a batch size which refer to the mini batches used for training a network. The following explains for which dataset and network type which iterator is necessary.\n\n\nDefaultInstanceIterator\n\n\nThe \nDefaultInstanceIterator\n assumes your dataset is of the following shape\n\n\n@RELATION iris\n\n@ATTRIBUTE sepallength  REAL\n@ATTRIBUTE sepalwidth   REAL\n@ATTRIBUTE petallength  REAL\n@ATTRIBUTE petalwidth   REAL\n@ATTRIBUTE class        {Iris-setosa,Iris-versicolor,Iris-virginica}\n\n@DATA\n5.1,3.5,1.4,0.2,Iris-setosa\n4.9,3.0,1.4,0.2,Iris-setosa\n4.7,3.2,1.3,0.2,Iris-setosa\n...\n\n\n\n\nthat is, each row is represented as a vector without further interpretation\nand you want to build a simple dense network of the form\n\n\nDenseLayer -\n DenseLayer -\n ... -\n OutputLayer\n\n\n\n\nConvolutionInstanceIterator\n\n\nTo use convolutional neural networks in the case of a more sophisticated dataset, where the ARFF file represents column-wise flattened image pixels as e.g.:\n\n\n@RELATION hotdog-3x3\n\n@ATTRIBUTE pixel00  REAL\n@ATTRIBUTE pixel01  REAL\n@ATTRIBUTE pixel02  REAL\n@ATTRIBUTE pixel10  REAL\n@ATTRIBUTE pixel11  REAL\n@ATTRIBUTE pixel12  REAL\n@ATTRIBUTE pixel20  REAL\n@ATTRIBUTE pixel21  REAL\n@ATTRIBUTE pixel22  REAL\n\n@ATTRIBUTE class    {hotdog, not-hotdog}\n\n@DATA\n127,32,15,234,214,144,94,43,23,hotdog\n52,14,244,232,241,11,142,211,211,not-hotdog\n...\n\n\n\n\nit is necessary to set the iterator to \nConvolutionInstanceIterator\n. \n\n\nAvailable parameters:\n\n\n\n\nheight\n: Height of the images\n\n\nwidth\n: Width of the images\n\n\nnumChannels\n: Depth of the image (e.g.: RGB images have a depth of 3, whereas Greyscale images have a depth of 1)\n\n\n\n\nImageInstanceIterator\n\n\nIf the dataset consists of a set of image files it is necessary to prepare a meta data ARFF file in the following format:\n\n\n@RELATION mnist.meta.minimal\n\n@ATTRIBUTE filename string\n@ATTRIBUTE class {0,1,2,3,4,5,6,7,8,9}\n\n@DATA\nimg_12829_0.jpg,0\nimg_32870_0.jpg,0\nimg_28642_0.jpg,0\n...\n\n\n\n\nThis file informs the internals about the association between the image files and their labels. Additionally it is mandatory to set the iterator to \nImageInstanceIterator\n. \n\n\nAvailable parameters:\n\n\n\n\nheight\n: Height of the images\n\n\nwidth\n: Width of the images\n\n\nnumChannels\n: Depth of the image (e.g.: RGB images have a depth of 3, whereas Greyscale images have a depth of 1)\n\n\nimagesLocation\n: The absolute path to the location of the images listed in the meta-data ARFF file\n\n\n\n\nTextEmbeddingInstanceIterator\n\n\nIf you are going to process text data, it is usually necessary to project the documents into an embedding space. This means, each token (e.g. a word) is mapped with the help of an embedding into a certain feature space. That is, each document will then contain a series of vectors, where each vector represents a token in the embedding space. The \nTextEmbeddingInstanceIterator\n accepts datasets containing a document and a class as shown below:\n\n\n@RELATION 'imdb-reviews'\n\n@ATTRIBUTE review string\n@ATTRIBUTE class-att {pos,neg}\n\n@data\n\nPrince stars as the Kid in this semi-autobiographical film ...\n,pos\n\nI just saw Behind Bedroom Doors and this was the first ...\n,neg\n...\n\n\n\n\nAvailable parameters:\n\n\n\n\nwordVectorLocation\n: File which provides the iterator with a serialized word embedding\n\n\nstopWords\n: Stopword strategy to filter unnecessary words\n\n\ntokenizerFactory\n: Defines how tokens are created\n\n\ntokenPreProcess\n: Defines how tokens are preprocessed\n\n\ntruncateLength\n: Maximum number of words per document\n\n\n\n\nTextFilesEmbeddingInstanceIterator\n\n\nThis iterator extends the \nTextEmbeddingInstanceIterator\n and allows the use of distributed documents that are not collected in a single ARFF file. Similar to the \nImageInstanceIterator\n, this iterator is applicable to an ARFF file containing the meta data, such as:\n\n\n@RELATION 'imdb-reviews'\n\n@ATTRIBUTE document-path string\n@ATTRIBUTE class-att {pos,neg}\n\n@data\nreview_0.txt,pos\nreview_1.txt,neg\n...\n\n\n\n\nAvailable parameters:\n\n\n\n\nwordVectorLocation\n: File which provides the iterator with a serialized word embedding\n\n\nstopWords\n: Stopword strategy to filter unnecessary words\n\n\ntokenizerFactory\n: Defines how tokens are created\n\n\ntokenPreProcess\n: Defines how tokens are preprocessed\n\n\ntruncateLength\n: Maximum number of words per document\n\n\ntextsLocation\n: The absolute path to the location of the text files listed in the meta data ARFF file\n\n\n\n\nCaching\n\n\nThe iterators allow to choose between three modes of caching:\n\n\n\n\nNONE\n: disable caches\n\n\nMEMORY\n: cache the generated mini batches in memory\n\n\nFILESYSTEM\n: cache the generated mini batches in the filesystem (in your system's temporary directory)\n\n\n\n\nThe cache will be built up in the first epoch. For further epochs, the batches do not need to be recomputed but are read from the cache. This might help if the batch generation is computation intensive.", 
            "title": "Loading Data"
        }, 
        {
            "location": "/user-guide/data/#loading-data", 
            "text": "The package provides so called  InstanceIterators  to load the given dataset in a correct shape. Each iterator allows to set a batch size which refer to the mini batches used for training a network. The following explains for which dataset and network type which iterator is necessary.", 
            "title": "Loading Data"
        }, 
        {
            "location": "/user-guide/data/#defaultinstanceiterator", 
            "text": "The  DefaultInstanceIterator  assumes your dataset is of the following shape  @RELATION iris\n\n@ATTRIBUTE sepallength  REAL\n@ATTRIBUTE sepalwidth   REAL\n@ATTRIBUTE petallength  REAL\n@ATTRIBUTE petalwidth   REAL\n@ATTRIBUTE class        {Iris-setosa,Iris-versicolor,Iris-virginica}\n\n@DATA\n5.1,3.5,1.4,0.2,Iris-setosa\n4.9,3.0,1.4,0.2,Iris-setosa\n4.7,3.2,1.3,0.2,Iris-setosa\n...  that is, each row is represented as a vector without further interpretation\nand you want to build a simple dense network of the form  DenseLayer -  DenseLayer -  ... -  OutputLayer", 
            "title": "DefaultInstanceIterator"
        }, 
        {
            "location": "/user-guide/data/#convolutioninstanceiterator", 
            "text": "To use convolutional neural networks in the case of a more sophisticated dataset, where the ARFF file represents column-wise flattened image pixels as e.g.:  @RELATION hotdog-3x3\n\n@ATTRIBUTE pixel00  REAL\n@ATTRIBUTE pixel01  REAL\n@ATTRIBUTE pixel02  REAL\n@ATTRIBUTE pixel10  REAL\n@ATTRIBUTE pixel11  REAL\n@ATTRIBUTE pixel12  REAL\n@ATTRIBUTE pixel20  REAL\n@ATTRIBUTE pixel21  REAL\n@ATTRIBUTE pixel22  REAL\n\n@ATTRIBUTE class    {hotdog, not-hotdog}\n\n@DATA\n127,32,15,234,214,144,94,43,23,hotdog\n52,14,244,232,241,11,142,211,211,not-hotdog\n...  it is necessary to set the iterator to  ConvolutionInstanceIterator .   Available parameters:   height : Height of the images  width : Width of the images  numChannels : Depth of the image (e.g.: RGB images have a depth of 3, whereas Greyscale images have a depth of 1)", 
            "title": "ConvolutionInstanceIterator"
        }, 
        {
            "location": "/user-guide/data/#imageinstanceiterator", 
            "text": "If the dataset consists of a set of image files it is necessary to prepare a meta data ARFF file in the following format:  @RELATION mnist.meta.minimal\n\n@ATTRIBUTE filename string\n@ATTRIBUTE class {0,1,2,3,4,5,6,7,8,9}\n\n@DATA\nimg_12829_0.jpg,0\nimg_32870_0.jpg,0\nimg_28642_0.jpg,0\n...  This file informs the internals about the association between the image files and their labels. Additionally it is mandatory to set the iterator to  ImageInstanceIterator .   Available parameters:   height : Height of the images  width : Width of the images  numChannels : Depth of the image (e.g.: RGB images have a depth of 3, whereas Greyscale images have a depth of 1)  imagesLocation : The absolute path to the location of the images listed in the meta-data ARFF file", 
            "title": "ImageInstanceIterator"
        }, 
        {
            "location": "/user-guide/data/#textembeddinginstanceiterator", 
            "text": "If you are going to process text data, it is usually necessary to project the documents into an embedding space. This means, each token (e.g. a word) is mapped with the help of an embedding into a certain feature space. That is, each document will then contain a series of vectors, where each vector represents a token in the embedding space. The  TextEmbeddingInstanceIterator  accepts datasets containing a document and a class as shown below:  @RELATION 'imdb-reviews'\n\n@ATTRIBUTE review string\n@ATTRIBUTE class-att {pos,neg}\n\n@data Prince stars as the Kid in this semi-autobiographical film ... ,pos I just saw Behind Bedroom Doors and this was the first ... ,neg\n...  Available parameters:   wordVectorLocation : File which provides the iterator with a serialized word embedding  stopWords : Stopword strategy to filter unnecessary words  tokenizerFactory : Defines how tokens are created  tokenPreProcess : Defines how tokens are preprocessed  truncateLength : Maximum number of words per document", 
            "title": "TextEmbeddingInstanceIterator"
        }, 
        {
            "location": "/user-guide/data/#textfilesembeddinginstanceiterator", 
            "text": "This iterator extends the  TextEmbeddingInstanceIterator  and allows the use of distributed documents that are not collected in a single ARFF file. Similar to the  ImageInstanceIterator , this iterator is applicable to an ARFF file containing the meta data, such as:  @RELATION 'imdb-reviews'\n\n@ATTRIBUTE document-path string\n@ATTRIBUTE class-att {pos,neg}\n\n@data\nreview_0.txt,pos\nreview_1.txt,neg\n...  Available parameters:   wordVectorLocation : File which provides the iterator with a serialized word embedding  stopWords : Stopword strategy to filter unnecessary words  tokenizerFactory : Defines how tokens are created  tokenPreProcess : Defines how tokens are preprocessed  truncateLength : Maximum number of words per document  textsLocation : The absolute path to the location of the text files listed in the meta data ARFF file", 
            "title": "TextFilesEmbeddingInstanceIterator"
        }, 
        {
            "location": "/user-guide/data/#caching", 
            "text": "The iterators allow to choose between three modes of caching:   NONE : disable caches  MEMORY : cache the generated mini batches in memory  FILESYSTEM : cache the generated mini batches in the filesystem (in your system's temporary directory)   The cache will be built up in the first epoch. For further epochs, the batches do not need to be recomputed but are read from the cache. This might help if the batch generation is computation intensive.", 
            "title": "Caching"
        }, 
        {
            "location": "/user-guide/rnns/", 
            "text": "Sequence Classification and Regression\n\n\nThe \nRnnSequenceClassifier\n allows for the construction of neural networks containing recurrent units. The following layer types are supported for these architectures:\n\n\n\n\nLSTM\n\n\nGravesLSTM\n\n\nRnnOutputLayer\n\n\n\n\nWorking with Text Data\n\n\nThis model is a good fit for classification and regression tasks on text data. Text can be interpreted as a sequence of so called \ntokens\n, where a token can be e.g. a character, word, sentence or even a whole document. These tokens can further be mapped with the help of an \nembedding\n into a vector space defined by the embedding. Therefore, a text document can be represented as a sequence of vectors. This can be achieved by using the \nTextEmbeddingInstanceIterator\n and providing an embedding that was previously downloaded (e.g. Google's pretrained News model from \nhere\n).", 
            "title": "Recurrent Neural Networks"
        }, 
        {
            "location": "/user-guide/rnns/#sequence-classification-and-regression", 
            "text": "The  RnnSequenceClassifier  allows for the construction of neural networks containing recurrent units. The following layer types are supported for these architectures:   LSTM  GravesLSTM  RnnOutputLayer", 
            "title": "Sequence Classification and Regression"
        }, 
        {
            "location": "/user-guide/rnns/#working-with-text-data", 
            "text": "This model is a good fit for classification and regression tasks on text data. Text can be interpreted as a sequence of so called  tokens , where a token can be e.g. a character, word, sentence or even a whole document. These tokens can further be mapped with the help of an  embedding  into a vector space defined by the embedding. Therefore, a text document can be represented as a sequence of vectors. This can be achieved by using the  TextEmbeddingInstanceIterator  and providing an embedding that was previously downloaded (e.g. Google's pretrained News model from  here ).", 
            "title": "Working with Text Data"
        }, 
        {
            "location": "/examples/classifying-iris/", 
            "text": "The Iris Dataset\n\n\nA very common dataset to test algorithms with is the \nIris Dataset\n . The following explains how to build a neural network from the command line, programmatically in java and in the Weka workbench GUI.\n\n\nThe iris dataset can be found in the \ndatasets/nominal\n directory of the WekaDeeplearning4j package.\n\n\n \n  \nIris Visualization \n\n  \n\n\n\n\nCommandline\n\n\nStarting simple, the most straight forward way to create a neural network with this package is by using the commandline. A Single-Layer-Perceptron (the most basic neural network possible) is shown in the following\n\n\n$ java -cp $WEKA_HOME/weka.jar weka.Run \\\n        .Dl4jMlpClassifier \\\n        -S 1 \\\n        -layer \nweka.dl4j.layers.OutputLayer \\\n                -activation weka.dl4j.activations.ActivationSoftmax \\\n                -lossFn weka.dl4j.lossfunctions.LossMCXENT\n \\\n        -config \nweka.dl4j.NeuralNetConfiguration \\\n                -updater weka.dl4j.updater.Adam\n \\\n        -numEpochs 10 \\\n        -t datasets/nominal/iris.arff \\\n        -split-percentage 66\n\n\n\n\nJava\n\n\nThe same architecture can be built programmatically with the following Java code\n\n\n// Create a new Multi-Layer-Perceptron classifier\nDl4jMlpClassifier clf = new Dl4jMlpClassifier();\n// Set a seed for reproducable results\nclf.setSeed(1);\n\n// Load the iris dataset and set its class index\nInstances data = new Instances(new FileReader(\ndatasets/nominal/iris.arff\n));\ndata.setClassIndex(data.numAttributes() - 1);\n\n// Define the output layer\nOutputLayer outputLayer = new OutputLayer();\noutputLayer.setActivationFn(new ActivationSoftmax());\noutputLayer.setLossFn(new LossMCXENT());\n\nNeuralNetConfiguration nnc = new NeuralNetConfiguration();\nnnc.setUpdater(new Adam());\n\n// Add the layers to the classifier\nclf.setLayers(new Layer[]{outputLayer});\nclf.setNeuralNetConfiguration(nnc);\n\n// Evaluate the network\nEvaluation eval = new Evaluation(data);\nint numFolds = 10;\neval.crossValidateModel(clf, data, numFolds, new Random(1));\n\nSystem.out.println(eval.toSummaryString());", 
            "title": "Classifying the Iris Dataset"
        }, 
        {
            "location": "/examples/classifying-iris/#the-iris-dataset", 
            "text": "A very common dataset to test algorithms with is the  Iris Dataset  . The following explains how to build a neural network from the command line, programmatically in java and in the Weka workbench GUI.  The iris dataset can be found in the  datasets/nominal  directory of the WekaDeeplearning4j package.   \n   Iris Visualization", 
            "title": "The Iris Dataset"
        }, 
        {
            "location": "/examples/classifying-iris/#commandline", 
            "text": "Starting simple, the most straight forward way to create a neural network with this package is by using the commandline. A Single-Layer-Perceptron (the most basic neural network possible) is shown in the following  $ java -cp $WEKA_HOME/weka.jar weka.Run \\\n        .Dl4jMlpClassifier \\\n        -S 1 \\\n        -layer  weka.dl4j.layers.OutputLayer \\\n                -activation weka.dl4j.activations.ActivationSoftmax \\\n                -lossFn weka.dl4j.lossfunctions.LossMCXENT  \\\n        -config  weka.dl4j.NeuralNetConfiguration \\\n                -updater weka.dl4j.updater.Adam  \\\n        -numEpochs 10 \\\n        -t datasets/nominal/iris.arff \\\n        -split-percentage 66", 
            "title": "Commandline"
        }, 
        {
            "location": "/examples/classifying-iris/#java", 
            "text": "The same architecture can be built programmatically with the following Java code  // Create a new Multi-Layer-Perceptron classifier\nDl4jMlpClassifier clf = new Dl4jMlpClassifier();\n// Set a seed for reproducable results\nclf.setSeed(1);\n\n// Load the iris dataset and set its class index\nInstances data = new Instances(new FileReader( datasets/nominal/iris.arff ));\ndata.setClassIndex(data.numAttributes() - 1);\n\n// Define the output layer\nOutputLayer outputLayer = new OutputLayer();\noutputLayer.setActivationFn(new ActivationSoftmax());\noutputLayer.setLossFn(new LossMCXENT());\n\nNeuralNetConfiguration nnc = new NeuralNetConfiguration();\nnnc.setUpdater(new Adam());\n\n// Add the layers to the classifier\nclf.setLayers(new Layer[]{outputLayer});\nclf.setNeuralNetConfiguration(nnc);\n\n// Evaluate the network\nEvaluation eval = new Evaluation(data);\nint numFolds = 10;\neval.crossValidateModel(clf, data, numFolds, new Random(1));\n\nSystem.out.println(eval.toSummaryString());", 
            "title": "Java"
        }, 
        {
            "location": "/examples/classifying-mnist/", 
            "text": "The MNIST Dataset\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe MNIST dataset provides images of handwritten digits of 10 classes (0-9) and suits the task of simple image classification. \n\n\nThe minimal MNIST arff file can be found in the \ndatasets/nominal\n directory of the WekaDeeplearning4j package. This arff file lists all images in \ndatasets/nominal/mnist-minimal\n and annotates their path with their class label.\n\n\nImportant note:\n The arff dataset contains two features, the first one being the \nfilename\n and the second one being the \nclass\n. Therefore it is necessary to define an \nImageDataSetIterator\n which uses these filenames in the directory given by the option \n-imagesLocation\n.\n\n\nCommandline\n\n\nThe following run creates a Conv(3x3x8) \n Pool(2x2,MAX) \n Conv(3x3x8) \n Pool(2x2,MAX) \n Out architecture\n\n\n$ java -Xmx5g -cp $WEKA_HOME/weka.jar weka.Run \\\n     .Dl4jMlpClassifier \\\n     -S 1 \\\n     -iterator \nweka.dl4j.iterators.instance.ImageInstanceIterator -imagesLocation datasets/nominal/mnist-minimal -numChannels 1 -height 28 -width 28 -bs 16\n \\\n     -normalization \nStandardize training data\n \\\n     -layer \nweka.dl4j.layers.ConvolutionLayer -nFilters 8 -activation weka.dl4j.activations.ActivationReLU -kernelSizeX 3 -kernelSizeY 3 -paddingX 0 -paddingY 0 -strideX 1 -strideY 1\n \\\n     -layer \nweka.dl4j.layers.SubsamplingLayer -kernelSizeX 2 -kernelSizeY 2 -paddingX 0 -paddingY 0 -poolingType MAX -strideX 1 -strideY 1\n \\\n     -layer \nweka.dl4j.layers.ConvolutionLayer -nFilters 8 -activation weka.dl4j.activations.ActivationReLU -kernelSizeX 3 -kernelSizeY 3 -paddingX 0 -paddingY 0 -strideX 1 -strideY 1\n \\\n     -layer \nweka.dl4j.layers.SubsamplingLayer -kernelSizeX 2 -kernelSizeY 2 -paddingX 0 -paddingY 0 -poolingType MAX -strideX 1 -strideY 1\n \\\n     -layer \nweka.dl4j.layers.OutputLayer -activation weka.dl4j.activations.ActivationSoftmax -lossFn weka.dl4j.lossfunctions.LossMCXENT\n \\\n     -config \nweka.dl4j.NeuralNetConfiguration -updater weka.dl4j.updater.Adam\n \\\n     -numEpochs 10 \\\n     -t datasets/nominal/mnist.meta.minimal.arff \\\n     -split-percentage 80\n\n\n\n\nJava\n\n\nThe same architecture can be built programmatically with the following Java code\n\n\n// Set up the MLP classifier\nDl4jMlpClassifier clf = new Dl4jMlpClassifier();\nclf.setSeed(1);\nclf.setNumEpochs(10);\n\n\n// Load the arff file\nInstances data = new Instances(new FileReader(\ndatasets/nominal/mnist.meta.minimal.arff\n));\ndata.setClassIndex(data.numAttributes() - 1);\n\n\n// Load the image iterator\nImageDataSetIterator imgIter = new ImageDataSetIterator();\nimgIter.setImagesLocation(new File(\ndatasets/nominal/mnist-minimal\n));\nimgIter.setHeight(28);\nimgIter.setWidth(28);\nimgIter.setNumChannels(1);\nimgIter.setTrainBatchSize(16);\nclf.setDataSetIterator(imgIter);\n\n\n// Setup the network layers\n// First convolution layer, 8 3x3 filter \nConvolutionLayer convLayer1 = new ConvolutionLayer();\nconvLayer1.setKernelSizeX(3);\nconvLayer1.setKernelSizeY(3);\nconvLayer1.setStrideX(1);\nconvLayer1.setStrideY(1);\nconvLayer1.setActivationFn(new ActivationReLU());\nconvLayer1.setNOut(8);\n\n// First maxpooling layer, 2x2 filter\nSubsamplingLayer poolLayer1 = new SubsamplingLayer();\npoolLayer1.setPoolingType(PoolingType.MAX);\npoolLayer1.setKernelSizeX(2);\npoolLayer1.setKernelSizeY(2);\npoolLayer1.setStrideX(1);\npoolLayer1.setStrideY(1);\n\n// Second convolution layer, 8 3x3 filter\nConvolutionLayer convLayer2 = new ConvolutionLayer();\nconvLayer2.setKernelSizeX(3);\nconvLayer2.setKernelSizeY(3);\nconvLayer2.setStrideX(1);\nconvLayer2.setStrideY(1);\nconvLayer2.setActivationFn(new ActivationReLU());\nconvLayer2.setNOut(8);\n\n// Second maxpooling layer, 2x2 filter\nSubsamplingLayer poolLayer2 = new SubsamplingLayer();\npoolLayer2.setPoolingType(PoolingType.MAX);\npoolLayer2.setKernelSizeX(2);\npoolLayer2.setKernelSizeY(2);\npoolLayer2.setStrideX(1);\npoolLayer2.setStrideY(1);\n\n// Output layer with softmax activation\nOutputLayer outputLayer = new OutputLayer();\noutputLayer.setActivationFn(new ActivationSoftmax());\noutputLayer.setLossFn(new LossMCXENT());\n\n\n// Set up the network configuration\nNeuralNetConfiguration nnc = new NeuralNetConfiguration();\nnnc.setUpdater(new Adam());\nclf.setNeuralNetConfiguration(nnc);\n\n\n// Set the layers\nclf.setLayers(new Layer[]{convLayer1, poolLayer1, convLayer2, poolLayer2, outputLayer});\n\n\n// Evaluate the network\nEvaluation eval = new Evaluation(data);\nint numFolds = 10;\neval.crossValidateModel(clf, data, numFolds, new Random(1));\n\nSystem.out.println(\n% Correct = \n + eval.pctCorrect());", 
            "title": "Classifying the MNIST Dataset"
        }, 
        {
            "location": "/examples/classifying-mnist/#the-mnist-dataset", 
            "text": "The MNIST dataset provides images of handwritten digits of 10 classes (0-9) and suits the task of simple image classification.   The minimal MNIST arff file can be found in the  datasets/nominal  directory of the WekaDeeplearning4j package. This arff file lists all images in  datasets/nominal/mnist-minimal  and annotates their path with their class label.  Important note:  The arff dataset contains two features, the first one being the  filename  and the second one being the  class . Therefore it is necessary to define an  ImageDataSetIterator  which uses these filenames in the directory given by the option  -imagesLocation .", 
            "title": "The MNIST Dataset"
        }, 
        {
            "location": "/examples/classifying-mnist/#commandline", 
            "text": "The following run creates a Conv(3x3x8)   Pool(2x2,MAX)   Conv(3x3x8)   Pool(2x2,MAX)   Out architecture  $ java -Xmx5g -cp $WEKA_HOME/weka.jar weka.Run \\\n     .Dl4jMlpClassifier \\\n     -S 1 \\\n     -iterator  weka.dl4j.iterators.instance.ImageInstanceIterator -imagesLocation datasets/nominal/mnist-minimal -numChannels 1 -height 28 -width 28 -bs 16  \\\n     -normalization  Standardize training data  \\\n     -layer  weka.dl4j.layers.ConvolutionLayer -nFilters 8 -activation weka.dl4j.activations.ActivationReLU -kernelSizeX 3 -kernelSizeY 3 -paddingX 0 -paddingY 0 -strideX 1 -strideY 1  \\\n     -layer  weka.dl4j.layers.SubsamplingLayer -kernelSizeX 2 -kernelSizeY 2 -paddingX 0 -paddingY 0 -poolingType MAX -strideX 1 -strideY 1  \\\n     -layer  weka.dl4j.layers.ConvolutionLayer -nFilters 8 -activation weka.dl4j.activations.ActivationReLU -kernelSizeX 3 -kernelSizeY 3 -paddingX 0 -paddingY 0 -strideX 1 -strideY 1  \\\n     -layer  weka.dl4j.layers.SubsamplingLayer -kernelSizeX 2 -kernelSizeY 2 -paddingX 0 -paddingY 0 -poolingType MAX -strideX 1 -strideY 1  \\\n     -layer  weka.dl4j.layers.OutputLayer -activation weka.dl4j.activations.ActivationSoftmax -lossFn weka.dl4j.lossfunctions.LossMCXENT  \\\n     -config  weka.dl4j.NeuralNetConfiguration -updater weka.dl4j.updater.Adam  \\\n     -numEpochs 10 \\\n     -t datasets/nominal/mnist.meta.minimal.arff \\\n     -split-percentage 80", 
            "title": "Commandline"
        }, 
        {
            "location": "/examples/classifying-mnist/#java", 
            "text": "The same architecture can be built programmatically with the following Java code  // Set up the MLP classifier\nDl4jMlpClassifier clf = new Dl4jMlpClassifier();\nclf.setSeed(1);\nclf.setNumEpochs(10);\n\n\n// Load the arff file\nInstances data = new Instances(new FileReader( datasets/nominal/mnist.meta.minimal.arff ));\ndata.setClassIndex(data.numAttributes() - 1);\n\n\n// Load the image iterator\nImageDataSetIterator imgIter = new ImageDataSetIterator();\nimgIter.setImagesLocation(new File( datasets/nominal/mnist-minimal ));\nimgIter.setHeight(28);\nimgIter.setWidth(28);\nimgIter.setNumChannels(1);\nimgIter.setTrainBatchSize(16);\nclf.setDataSetIterator(imgIter);\n\n\n// Setup the network layers\n// First convolution layer, 8 3x3 filter \nConvolutionLayer convLayer1 = new ConvolutionLayer();\nconvLayer1.setKernelSizeX(3);\nconvLayer1.setKernelSizeY(3);\nconvLayer1.setStrideX(1);\nconvLayer1.setStrideY(1);\nconvLayer1.setActivationFn(new ActivationReLU());\nconvLayer1.setNOut(8);\n\n// First maxpooling layer, 2x2 filter\nSubsamplingLayer poolLayer1 = new SubsamplingLayer();\npoolLayer1.setPoolingType(PoolingType.MAX);\npoolLayer1.setKernelSizeX(2);\npoolLayer1.setKernelSizeY(2);\npoolLayer1.setStrideX(1);\npoolLayer1.setStrideY(1);\n\n// Second convolution layer, 8 3x3 filter\nConvolutionLayer convLayer2 = new ConvolutionLayer();\nconvLayer2.setKernelSizeX(3);\nconvLayer2.setKernelSizeY(3);\nconvLayer2.setStrideX(1);\nconvLayer2.setStrideY(1);\nconvLayer2.setActivationFn(new ActivationReLU());\nconvLayer2.setNOut(8);\n\n// Second maxpooling layer, 2x2 filter\nSubsamplingLayer poolLayer2 = new SubsamplingLayer();\npoolLayer2.setPoolingType(PoolingType.MAX);\npoolLayer2.setKernelSizeX(2);\npoolLayer2.setKernelSizeY(2);\npoolLayer2.setStrideX(1);\npoolLayer2.setStrideY(1);\n\n// Output layer with softmax activation\nOutputLayer outputLayer = new OutputLayer();\noutputLayer.setActivationFn(new ActivationSoftmax());\noutputLayer.setLossFn(new LossMCXENT());\n\n\n// Set up the network configuration\nNeuralNetConfiguration nnc = new NeuralNetConfiguration();\nnnc.setUpdater(new Adam());\nclf.setNeuralNetConfiguration(nnc);\n\n\n// Set the layers\nclf.setLayers(new Layer[]{convLayer1, poolLayer1, convLayer2, poolLayer2, outputLayer});\n\n\n// Evaluate the network\nEvaluation eval = new Evaluation(data);\nint numFolds = 10;\neval.crossValidateModel(clf, data, numFolds, new Random(1));\n\nSystem.out.println( % Correct =   + eval.pctCorrect());", 
            "title": "Java"
        }, 
        {
            "location": "/examples/classifying-imdb/", 
            "text": "The IMDB Dataset\n\n\nAs of \nai.stanford.edu\n:\n\n\n\n\nThis is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well. Raw text and already processed bag of words formats are provided. See the README file contained in the release for more details.\n\n\n\n\nThe full IMDB dataset in the ARFF format can be found \nhere\n.\n\n\nJava\n\n\nThe following code builds a network consisting of an LSTM layer and an RnnOutputLayer, loading imdb reviews and mapping them into a sequence of vectors in the embedding space that is defined by the Google News model. Furthermore, gradient clipping at a value of 1.0 is applied to prevent the network from exploding gradients.\n\n\n// Download e.g the SLIM Google News model from\n// https://github.com/eyaler/word2vec-slim/raw/master/GoogleNews-vectors-negative300-SLIM.bin.gz\nfinal File modelSlim = new File(\npath/to/google/news/model\n);\n\n// Setup hyperparameters\nfinal int truncateLength = 80;\nfinal int batchSize = 64;\nfinal int seed = 1;\nfinal int numEpochs = 10;\nfinal int tbpttLength = 20;\nfinal double l2 = 1e-5;\nfinal double gradientThreshold = 1.0;\nfinal double learningRate = 0.02;\n\n// Setup the iterator\nTextEmbeddingInstanceIterator tii = new TextEmbeddingInstanceIterator();\ntii.setWordVectorLocation(modelSlim);\ntii.setTruncateLength(truncateLength);\ntii.setTrainBatchSize(batchSize);\n\n// Initialize the classifier\nRnnSequenceClassifier clf = new RnnSequenceClassifier();\nclf.setSeed(seed);\nclf.setNumEpochs(numEpochs);\nclf.setInstanceIterator(tii);\nclf.settBPTTbackwardLength(tbpttLength);\nclf.settBPTTforwardLength(tbpttLength);\n\n// Define the layers\nLSTM lstm = new LSTM();\nlstm.setNOut(64);\nlstm.setActivationFunction(new ActivationTanH());\n\nRnnOutputLayer rnnOut = new RnnOutputLayer();\n\n// Network config\nNeuralNetConfiguration nnc = new NeuralNetConfiguration();\nnnc.setL2(l2);\nnnc.setUseRegularization(true);\nnnc.setGradientNormalization(GradientNormalization.ClipElementWiseAbsoluteValue);\nnnc.setGradientNormalizationThreshold(gradientThreshold);\nnnc.setLearningRate(learningRate);\n\n// Config classifier\nclf.setLayers(lstm, rnnOut);\nclf.setNeuralNetConfiguration(nnc);\nInstances data = new Instances(new FileReader(\nsrc/test/resources/nominal/imdb.arff\n));\ndata.setClassIndex(1);\nclf.buildClassifier(data);", 
            "title": "Classifying the IMDB Dataset"
        }, 
        {
            "location": "/examples/classifying-imdb/#the-imdb-dataset", 
            "text": "As of  ai.stanford.edu :   This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well. Raw text and already processed bag of words formats are provided. See the README file contained in the release for more details.   The full IMDB dataset in the ARFF format can be found  here .", 
            "title": "The IMDB Dataset"
        }, 
        {
            "location": "/examples/classifying-imdb/#java", 
            "text": "The following code builds a network consisting of an LSTM layer and an RnnOutputLayer, loading imdb reviews and mapping them into a sequence of vectors in the embedding space that is defined by the Google News model. Furthermore, gradient clipping at a value of 1.0 is applied to prevent the network from exploding gradients.  // Download e.g the SLIM Google News model from\n// https://github.com/eyaler/word2vec-slim/raw/master/GoogleNews-vectors-negative300-SLIM.bin.gz\nfinal File modelSlim = new File( path/to/google/news/model );\n\n// Setup hyperparameters\nfinal int truncateLength = 80;\nfinal int batchSize = 64;\nfinal int seed = 1;\nfinal int numEpochs = 10;\nfinal int tbpttLength = 20;\nfinal double l2 = 1e-5;\nfinal double gradientThreshold = 1.0;\nfinal double learningRate = 0.02;\n\n// Setup the iterator\nTextEmbeddingInstanceIterator tii = new TextEmbeddingInstanceIterator();\ntii.setWordVectorLocation(modelSlim);\ntii.setTruncateLength(truncateLength);\ntii.setTrainBatchSize(batchSize);\n\n// Initialize the classifier\nRnnSequenceClassifier clf = new RnnSequenceClassifier();\nclf.setSeed(seed);\nclf.setNumEpochs(numEpochs);\nclf.setInstanceIterator(tii);\nclf.settBPTTbackwardLength(tbpttLength);\nclf.settBPTTforwardLength(tbpttLength);\n\n// Define the layers\nLSTM lstm = new LSTM();\nlstm.setNOut(64);\nlstm.setActivationFunction(new ActivationTanH());\n\nRnnOutputLayer rnnOut = new RnnOutputLayer();\n\n// Network config\nNeuralNetConfiguration nnc = new NeuralNetConfiguration();\nnnc.setL2(l2);\nnnc.setUseRegularization(true);\nnnc.setGradientNormalization(GradientNormalization.ClipElementWiseAbsoluteValue);\nnnc.setGradientNormalizationThreshold(gradientThreshold);\nnnc.setLearningRate(learningRate);\n\n// Config classifier\nclf.setLayers(lstm, rnnOut);\nclf.setNeuralNetConfiguration(nnc);\nInstances data = new Instances(new FileReader( src/test/resources/nominal/imdb.arff ));\ndata.setClassIndex(1);\nclf.buildClassifier(data);", 
            "title": "Java"
        }, 
        {
            "location": "/troubleshooting/", 
            "text": "This section shall provide solutions for issues that  may appear.\n\n\n\n\nCUDA: GOMP Version 4.0 not found\n\n\nIssue:\n Starting the \nDl4jMlpClassifier\n while using the GPU version of the package results in something similar to:\n\n\nCaused by: java.lang.UnsatisfiedLinkError: \n    /home/user/.javacpp/cache/nd4j-cuda-8.0-0.9.1-linux-x86_64.jar/org/nd4j/nativeblas/linux-x86_64/libjnind4jcuda.so: \n    /usr/lib/x86_64-linux-gnu/libgomp.so.1: \n    version `GOMP_4.0' not found \n    (required by /home/user/.javacpp/cache/nd4j-cuda-8.0-0.9.1-linux-x86_64.jar/org/nd4j/nativeblas/linux-x86_64/libnd4jcuda.so)\n\n\n\n\n\nThis happens when your system is using a version below 4.9 of the gcc compiler. You can check this with:\n\n\n$ gcc --version\ngcc (Ubuntu 4.8.4-2ubuntu1~14.04.3) 4.8.4\nCopyright (C) 2013 Free Software Foundation, Inc.\nThis is free software; see the source for copying conditions.  There is NO\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n\n\n\n\nTherefore the libgomp.so.1 library is still of version 3.0, while the nd4j backend expects version 4.0.\n\n\nSolution\n: Download the latest version of libgomp for your system and export the following:\n\n\nexport LD_PRELOAD=\nPATH-TO-NEW-LIBGOMP.SO\n\n\n\n\n\nFor Ubuntu you can get the library \nhere\n, choose your architecture, download and extract the deb-file. For amd64 architectures this would be:\n\n\n$ wget http://security.ubuntu.com/ubuntu/pool/main/g/gcc-5/libgomp1_5.4.0-6ubuntu1~16.04.4_amd64.deb\n$ ar vx libgomp1_5.4.0-6ubuntu1~16.04.4_amd64.deb\n$ tar -xvf data.tar.xz\n\n\n\n\nThis extracts the library to \n./usr/lib/x86_64-linux-gnu/libgomp.so.1\n. Afterward set the \nLD_PRELOAD\n variable to this path as an absolute path and export it as shown above.\n\n\n\n\nCUDA: Failed to allocate X bytes from DEVICE memory\n\n\nIssue:\n Your network architecture or your batch size consumes too much memory.\n\n\nSolution\n: Use a lower batch size, or adjust your Java heap and off-heap limits to your available memory accordingly to the \nofficial Dl4J memory description\n.", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/troubleshooting/#cuda-gomp-version-40-not-found", 
            "text": "Issue:  Starting the  Dl4jMlpClassifier  while using the GPU version of the package results in something similar to:  Caused by: java.lang.UnsatisfiedLinkError: \n    /home/user/.javacpp/cache/nd4j-cuda-8.0-0.9.1-linux-x86_64.jar/org/nd4j/nativeblas/linux-x86_64/libjnind4jcuda.so: \n    /usr/lib/x86_64-linux-gnu/libgomp.so.1: \n    version `GOMP_4.0' not found \n    (required by /home/user/.javacpp/cache/nd4j-cuda-8.0-0.9.1-linux-x86_64.jar/org/nd4j/nativeblas/linux-x86_64/libnd4jcuda.so)  This happens when your system is using a version below 4.9 of the gcc compiler. You can check this with:  $ gcc --version\ngcc (Ubuntu 4.8.4-2ubuntu1~14.04.3) 4.8.4\nCopyright (C) 2013 Free Software Foundation, Inc.\nThis is free software; see the source for copying conditions.  There is NO\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  Therefore the libgomp.so.1 library is still of version 3.0, while the nd4j backend expects version 4.0.  Solution : Download the latest version of libgomp for your system and export the following:  export LD_PRELOAD= PATH-TO-NEW-LIBGOMP.SO   For Ubuntu you can get the library  here , choose your architecture, download and extract the deb-file. For amd64 architectures this would be:  $ wget http://security.ubuntu.com/ubuntu/pool/main/g/gcc-5/libgomp1_5.4.0-6ubuntu1~16.04.4_amd64.deb\n$ ar vx libgomp1_5.4.0-6ubuntu1~16.04.4_amd64.deb\n$ tar -xvf data.tar.xz  This extracts the library to  ./usr/lib/x86_64-linux-gnu/libgomp.so.1 . Afterward set the  LD_PRELOAD  variable to this path as an absolute path and export it as shown above.", 
            "title": "CUDA: GOMP Version 4.0 not found"
        }, 
        {
            "location": "/troubleshooting/#cuda-failed-to-allocate-x-bytes-from-device-memory", 
            "text": "Issue:  Your network architecture or your batch size consumes too much memory.  Solution : Use a lower batch size, or adjust your Java heap and off-heap limits to your available memory accordingly to the  official Dl4J memory description .", 
            "title": "CUDA: Failed to allocate X bytes from DEVICE memory"
        }
    ]
}